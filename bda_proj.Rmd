---
title: 'MATH 264: Bayesian Project'
output:
  html_document: default
  html_notebook: default
---

```{r, echo = FALSE}
## Reading in datasets for the analysis

## Death count data (Quentin Taratino movies removed)
dc <- read.csv("filmdeathcounts_no-qt.csv", 
               header = TRUE, 
               stringsAsFactors = FALSE)
dc$Kill_Rate <- dc$Body_Count/dc$Length_Minutes * 60
## Project dataset meta data
qt <- read.table("qt_data.txt", 
                 header = FALSE, 
                 col.names = c("movie", "body.count", "rating", "hours", "year", "genre"),
                 sep = ";",
                 strip.white = TRUE,
                 stringsAsFactors = FALSE)
qt$kill.rate <- round(qt$body.count / qt$hours, 2)
qt$model.1 <- c(1,1,1,0,1,1,0,0,1)
qt$model.2 <- c(0,0,0,1,0,0,0,1,0)
qt$model.3 <- c(0,0,0,0,0,0,1,0,0)
qt <- qt[, c(1,6,5,3,2,4,7,8,9,10)]
```

---

# I. Data Summary  

---

> "A few numbers are approximated due to the impossibility of counting precisely how many ninjas are decapitated in **Kill Bill Vol. 1**, how many Nazis are in the theater when it gets set afire in **Inglorious Basterds**, and how many people fall in the never-ending shoot-out scene at the end of **Django Unchained**."  
> - [Vanity Fair](http://www.vanityfair.com/hollywood/2013/02/quentin-tarantino-deaths-movies)

### Taratino Movie Data  
Meta data extracted from [IMDB](http://www.imdb.com/)

```{r, echo = FALSE}
knitr::kable(qt, align = "l")
```

### Movie Database Sample  

* Filter conditions:
    - year: 1989-2013
    - rating: R, Unrated, or NR
    - genre:
        - Model 1: crime, drama, thriller, action, western, mystery
        - Model 2: action, thriller, drama, western
        - Model 3: war
    - director: not Quentin
* Dataset does not contain movies with 0 deaths. This ensures the kill rate per hour is greater than 0, which is appropriate for the support of Gamma.  


```{r, echo = FALSE}
qt.all <- qt
qt.m1 <- qt[qt$model.1 == 1, ]
qt.m2 <- qt[qt$model.2 == 1, ]
qt.m3 <- qt[qt$model.3 == 1, ]

## Genre list for project dataset
qt.g.m1 <- unique(unlist(strsplit(paste(qt.m1$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m2 <- unique(unlist(strsplit(paste(qt.m2$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m3 <- unique(unlist(strsplit(paste(qt.m3$genre, collapse = ", ", sep = ""), ", ")))

## Splitting genre list for each movie in death count dataset
dc.g <- strsplit(tolower(dc$Genre), "|", fixed = TRUE)

## Checking if any genres match those of project genre list
dc$Genre_Match_M1 <- logical(1)
dc$Genre_Match_M2 <- logical(1)
dc$Genre_Match_M3 <- logical(1)
for (i in seq_along(dc.g)) {
  dc$Genre_Match_M1[i] <- (all(dc.g[[i]] %in% qt.g.m1)) | (sum(dc.g[[i]] %in% qt.g.m1) >= 3)
  dc$Genre_Match_M2[i] <- (all(dc.g[[i]] %in% qt.g.m2)) | (sum(dc.g[[i]] %in% qt.g.m2) >= 3)
  dc$Genre_Match_M3[i] <- all(dc.g[[i]] %in% qt.g.m3)
}

## Filter death count dataset
dc.filt.m1 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M1 == TRUE, ]
dc.filt.m2 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M2 == TRUE, ]
dc.filt.m3 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_War == 1, ]
genre.sum <- round(data.frame(rbind(length(dc.filt.m1$Kill_Rate), length(dc.filt.m2$Kill_Rate), length(dc.filt.m3$Kill_Rate)), 
                        rbind(summary(dc.filt.m1$Kill_Rate), summary(dc.filt.m2$Kill_Rate), summary(dc.filt.m3$Kill_Rate)),
                        row.names = c("Model 1", "Model 2", "Model 3")), 2)

#names(genre.sum) <- c("N", "Min", "Q.1", "Median", "Mean", "Q.3", "Max")
knitr::kable(genre.sum, align = "l")
```


---

# II. Models

---

### Sampling Distribution: Poisson with rate and exposure

According to Gelman et. al. (pg. 45), this model is NOT exchangeable in the $y_i$'s but is exchangeable in the pairs $(x,y)_i$

##### Assumptions

Lecture (See slide 23 of LectureD):  

* let $y(t)$ denote the number of events that have occurred during a time interval $[0, t]$
* P1: $y(0) = 0$
* P2: For all $n \geq 0$, and for any two time intervals, $I_1$ and $I_2$, of equal length, Pr($n$ events in $I_1$) = Pr($n$ events in $I_2$)
* P3: Events that occur in nonoverlapping time intervals are mutually independent (want to relax this and replace with exchangeability)
* P4: $\lim\limits_{h \to 0}\frac{\text{Pr}(y(h) > 1)}{h} = 0$
* P5: $0 < \text{Pr}\{y(t)=0\} < 1, ~ \forall ~  t > 0$

Under these conditions, there exists a positive number $\theta$ that produces the density below where $\theta$ = the true underlying death rate in Quentin Taratino movies (measured in bodies per hour)

$$p(y ~|~ \theta) = \frac{1}{y!}(\theta t)^{y}e^{-(\theta t)} \cdot {1}_{\{0,1,2,...\}}(y)$$

Wikipedia:  

* $y$ the number of times an event occurs in an interval and it can take values 0, 1, 2,...
* The occurrence of one event does not affect the probability that a second event will occur. That is, events occur independently.
* The rate at which events occur is constant. The rate cannot be higher in some intervals and lower in other intervals.
* Two events cannot occur at exactly the same instant.
* The probability of an event in an interval is proportional to the length of the interval.

Not an assumption but a result of the sampling distribution: E[$y(t) | \theta$] = Var($y(t) | \theta$) = $\theta \cdot t$
  
### Likelihood
A necessary and sufficient condition to get product of identical distributions is  
$$p(y_1,...y_n ~|~ s_n) = \frac{s_n!}{y_1!,...,y_n!} \prod\limits_{i=1}^n \left(\frac{1}{n}\right)^{y_i}$$
For every $n$, where $s_n = y_1 + ... + y_n$. This condition is not reasonably justified if we group all the data together; however, once grouped into three models, the condition seems reasonable within each model. Assuming this condition holds, the likelihood is:

$$p(y ~|~ \theta) \propto \theta^{\left(\sum\limits_{i=1}^n y_i\right)} \exp\left( -\theta \sum\limits_{i=1}^n t_i \right)$$

-----------------------------------------
Model   Likelihood: $p(y ~|~ \theta)$
------  ------------------
 1       $\propto \theta^{59}e^{-13.73\theta}$
  
 2       $\propto \theta^{126}e^{-4.6\theta}$
  
 3       $\propto \theta^{396}e^{-2.55\theta}$
-----------------------------------------
    
### Conjugate Prior: Gamma

[Data](https://figshare.com/articles/On_screen_movie_kill_counts_for_hundreds_of_films/889719) compiled by [Randal Olson](http://www.randalolson.com/) from the site: http://www.moviebodycounts.com/  

Since the conjugate prior distribution for the Poisson sampling distribution is Gamma, the prior distribution of $\theta$ will be of the form:  
$$p(\theta) \propto \theta^{\alpha - 1}e^{-\beta\cdot\theta}$$

##### Model 1:

```{r, echo = FALSE}
## Conjugate prior stuff
h <- 5
alpha.m1 <- 1.46
beta.m1 <- 0.054

d1 <- density(dc.filt.m1$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 200)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 200), ylim = c(0, 0.06), 
     bty = "n", type = "n")
hist(dc.filt.m1$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d1, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 200, 50))
axis(2, at = seq.int(0, 0.06, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 200, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m1, ", ", beta.m1,")", sep = ""), "Kernel density estimate"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")
```

The parameters for the Gamma distribution were determined by solving the system of equations generated by setting the mode equal to 8.61 and by observing that approximately 99.9% of the data is in the range (0,150).  

##### Model 2: 

```{r, echo = FALSE}
## Conjugate prior stuff
h <- 10
alpha.m2 <- 2.13
beta.m2 <- 0.064

d2 <- density(dc.filt.m2$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 160)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 160), ylim = c(0, 0.06), 
     bty = "n", type = "n")
hist(dc.filt.m2$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d2, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 160, 20))
axis(2, at = seq.int(0, 0.06, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), from = 0, to = 160, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m2, ", ", beta.m2,")", sep = ""), "Kernel density estimate"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")
```

The parameters for the Gamma distribution were determined by solving the system of equations generated by setting the mode equal to 17.85 and by observing that approximately 99.9% of the data is in the range (0,150).  

##### Model 3: 

```{r, echo = FALSE}
## Conjugate prior stuff
h <- 30
alpha.m3 <- 2.015
beta.m3 <- 0.023

d3 <- density(dc.filt.m3$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 400)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 400), ylim = c(0, 0.02), 
     bty = "n", type = "n")
hist(dc.filt.m3$Kill_Rate, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d3, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 400, 50), labels = NA)
axis(1, at = seq.int(0, 400, 100), lwd = 0, lwd.ticks = 0)
axis(2, at = seq.int(0, 0.02, 0.005))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), from = 0, to = 400, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m3, ", ", beta.m3,")", sep = ""), "Kernel density estimate"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
```

The parameters for the Gamma distribution were determined by solving the system of equations generated by setting the mode equal to 43.83 and by observing that approximately 99.9% of the data is in the range (0,400).  


Model |  Prior: $p(\theta)$ | Named
------|---------------------|---------
 1 | $\propto \theta^{1.46 - 1}e^{-(0.053)\theta}$ | Gamma(1.46, 0.053)
 2 | $\propto \theta^{2.13 - 1}e^{-(0.064)\theta}$ | Gamma(2.13, 0.064)
 3 | $\propto \theta^{2.015 - 1}e^{-(0.023)\theta}$ | Gamma(2.015, 0.023)



### Posterior Distribution  

In general, the posterior distribution is Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$

```{r, echo = FALSE}
t.sum.m1 <- sum(qt.m1$hours)
y.sum.m1 <- sum(qt.m1$body.count)
alpha.post.m1 <- alpha.m1 + y.sum.m1
beta.post.m1 <- beta.m1 + t.sum.m1
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), from = 0, to = 50, n = 500, lty = 1, col = "violetred", ann = FALSE, bty = "n")
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 50, n = 500, lty = 2, col = "slateblue", add = TRUE)
legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")
```

```{r, echo = FALSE}
t.sum.m2 <- sum(qt.m2$hours)
y.sum.m2 <- sum(qt.m2$body.count)
alpha.post.m2 <- alpha.m2 + y.sum.m2
beta.post.m2 <- beta.m2 + t.sum.m2
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), from = 0, to = 100, n = 500, lty = 1, col = "violetred", ann = FALSE, bty = "n")
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), from = 0, to = 100, n = 500, lty = 2, col = "slateblue", add = TRUE)
legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")
```

```{r, echo = FALSE}
t.sum.m3 <- sum(qt.m3$hours)
y.sum.m3 <- sum(qt.m3$body.count)
alpha.post.m3 <- alpha.m3 + y.sum.m3
beta.post.m3 <- beta.m3 + t.sum.m3
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), from = 0, to = 400, n = 1000, lty = 1, col = "violetred", ann = FALSE, bty = "n")
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), from = 0, to = 400, n = 1000, lty = 2, col = "slateblue", add = TRUE)
legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
```

### TODO: Posterior Tables

##### Highest Posterior Density Interval: 99%

```{r, echo = FALSE}
p <- 0.99
hpd.m1 <- HDInterval::hdi(qgamma, shape = alpha.post.m1, rate = beta.post.m1, credMass = p)
hpd.m2 <- HDInterval::hdi(qgamma, shape = alpha.post.m2, rate = beta.post.m2, credMass = p)
hpd.m3 <- HDInterval::hdi(qgamma, shape = alpha.post.m3, rate = beta.post.m3, credMass = p)
hpd.df <- round(data.frame(rbind(hpd.m1, hpd.m2, hpd.m3), row.names = c("Model 1", "Model 2", "Model 3")), 2)
names(hpd.df) <- c("Lower Bound", "Upper Bound")
knitr::kable(hpd.df, align = "l")
```

### Posterior Predictive Distribution

The posterior predictive distribution for a single additional observation is a negative binomial distribution (see Gelman page 44-45). Assuming conditional independence of y and $\tilde{y}$ given $\theta$.  
\begin{align*}
p(\tilde{y} ~|~ y) & = \int\limits_0^{\infty} p(\tilde{y} ~|~ \theta)\cdot p(\theta ~|~ y) ~d\theta \\
& = \int\limits_0^{\infty} \text{Poisson}(\tilde{y}(t) ~|~ \theta) \text{Gamma}\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right) ~d\theta \\
& = \text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)
\end{align*}

```{r, echo = FALSE}
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd.m1 <- beta.post.m1/t.new
post.preds <- mapply(dnbinom, x = 0:35, MoreArgs = list(size = rep(alpha.post.m1, length(t.new)), prob = (beta.ppd.m1/(beta.ppd.m1 + 1))))
x.modes.m1 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m1 <- post.preds[matrix(c(1:4, x.modes.m1), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 35), ylim = c(0, 0.16))
for (i in 1:4) {
  lines(0:35, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m1[i] - 1, 2), c(-0.1, modes.m1[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,35,5), col = "gray50")
axis(2, seq.int(0,0.16,0.04), col = "gray50")
title(main = "Model 1", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")))
legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m1 + 3, modes.m1 + 0.04, labels = round(modes.m1, 3), pos = 4)
#arrows(x.modes.m1 - 1, modes.m1, x.modes.m1 + 3, modes.m1 + 0.04, length = 0.1, code = 1)
points(x.modes.m1 - 1, rep(0, length(x.modes.m1)), pch = 4, col = "black")
```


```{r, echo = FALSE}
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd.m2 <- beta.post.m2/t.new
post.preds <- mapply(dnbinom, x = 0:140, MoreArgs = list(size = rep(alpha.post.m2, length(t.new)), prob = (beta.ppd.m2/(beta.ppd.m2 + 1))))
x.modes.m2 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m2 <- post.preds[matrix(c(1:4, x.modes.m2), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 140), ylim = c(0, 0.06))
for (i in 1:4) {
  lines(0:140, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m2[i] - 1, 2), c(-0.1, modes.m2[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,140,20), col = "gray50")
axis(2, seq.int(0,0.06,0.02), col = "gray50")
title(main = "Model 2", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")))
legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m2 + 3, modes.m2 + 0.04, labels = round(modes.m2, 3), pos = 4)
#arrows(x.modes.m2 - 1, modes.m2, x.modes.m2 + 3, modes.m2 + 0.04, length = 0.1, code = 1)
points(x.modes.m2 - 1, rep(0, length(x.modes.m2)), pch = 4, col = "black")
```

```{r, echo = FALSE}
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd.m3 <- beta.post.m3/t.new
post.preds <- mapply(dnbinom, x = 0:600, MoreArgs = list(size = rep(alpha.post.m3, length(t.new)), prob = (beta.ppd.m3/(beta.ppd.m3 + 1))))
x.modes.m3 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m3 <- post.preds[matrix(c(1:4, x.modes.m3), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(150, 600), ylim = c(0, 0.024))
for (i in 1:4) {
  lines(0:600, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m3[i] - 1, 2), c(-0.1, modes.m3[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,600,50), col = "gray50")
axis(2, seq.int(0,0.024,0.006), col = "gray50")
title(main = "Model 3", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")))
legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m3 + 3, modes.m3 + 0.04, labels = round(modes.m3, 3), pos = 4)
#arrows(x.modes.m3 - 1, modes.m3, x.modes.m3 + 3, modes.m3 + 0.04, length = 0.1, code = 1)
points(x.modes.m3 - 1, rep(0, length(x.modes.m3)), pch = 4, col = "black")
```

Thus, the most likely values for the body count in Quentin Taratino's next movie are:

```{r, echo = FALSE}
ppd.df <- data.frame(hours = t.new, m1 = (x.modes.m1 - 1), m1p = round(modes.m1, 3), m2 = (x.modes.m2 - 1), m2p = round(modes.m2, 3), m3 = (x.modes.m3 - 1), m3p = round(modes.m3, 3))
names(ppd.df) <- c("Movie Duration (hours)", "Model 1: Body Count", "Model 1: Pr(y)", "Model 2: Body Count", "Model 2: Pr(y)", "Model 3: Body Count", "Model 3: Pr(y)")
knitr::kable(ppd.df, align = "l")
```

---

# III. Model Checking  

---

### Sensitivity to Prior Distribution

In this section we analyze the effects of different prior distributions on posterior inference. First we consider the effects of a noninformative prior instead of the gamma conjugate prior. The Jeffreys prior for $\theta$ follows from the Fisher information calculation below:  
$$\begin{align*}
I_n(\theta) & = \text{E}\left\{ \left(\frac{\partial}{\partial \theta} \log p(y~|~\theta)\right)^2 ~\Bigg|~ \theta \right\} \\
& = \frac{1}{\theta^2} \text{E}\left[\left(\sum\limits_{i=1}^n y_i - \theta\sum\limits_{i=1}^n t_i\right)^2 ~\Bigg|~ \theta \right] \\
& = \frac{1}{\theta^2} \text{Var}\left( \sum\limits_{i=1}^n y_i ~\Bigg|~ \theta \right) \\
& = \frac{1}{\theta} \cdot \sum\limits_{i=1}^n t_i \\
\implies p(\theta) & \propto \sqrt{\frac{1}{\theta}}
\end{align*}$$  
This results in an improper prior; however, this does not prevent us from finding a posterior distribution for $\theta$ [WHY???]. The resulting posterior distribution for $\theta$ is Gamma$\left(0.5 + \sum\limits_{i=1}^n y_i, \sum\limits_{i=1}^n t_i\right)$. 

Below are the plots of the posterior distributions of $\theta$ as a result of using a conjugate prior and a non-informative prior. In all three models the differences are neglible.

```{r, echo = FALSE}
par(mfrow = c(1,3))
curve(dgamma(x, shape = y.sum.m1 + 0.5, rate = t.sum.m1), 
      from = 0, to = 10, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(0,10), ylim = c(0,1))
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), 
      from = 0, to = 10, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")



curve(dgamma(x, shape = y.sum.m2 + 0.5, rate = t.sum.m2), 
      from = 10, to = 50, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(10,50), ylim = c(0,0.3))
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), 
      from = 10, to = 50, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
legend("top", legend = c("Conjugate", "Noninformative"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")


curve(dgamma(x, shape = y.sum.m3 + 0.5, rate = t.sum.m3), 
      from = 120, to = 200, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(120,200), ylim = c(0,0.08))
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), 
      from = 120, to = 200, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```



[TODO!!!]
Additionally, we will examine the effect of choosing a conjugate gamma prior distribution with larger variance to account for higher body counts.

### Posterior Predictive Checking  
The observed data should look plausible under the posterior predictive distribution
```{r, echo = FALSE}
plot(qt$hours, qt$body.count, ann = FALSE, axes = FALSE, xlim = c(1.5, 3), ylim = c(0,20), col = "violetred", pch = 4)
#box(col = "gray50", lty = 1)
axis(1, at = seq.int(1.5, 3, 0.5), col = "gray50")
axis(2, at = seq.int(0,20,5), col = "gray50")
title(xlab = "Movie Duration (hours)", ylab = "Body Count")
```

Correlation between body count and duration: `r round(cor(qt$hours, qt$body.count), 2)`  

```{r}
## Simulation set=up
set.seed(2016)
m <- 500000
n.obs <- length(qt.m1$body.count)
theta.sim <- rgamma(m, shape = alpha.post.m1, rate = beta.post.m1)
t.sim <- runif(m, min = min(qt.m1$hours), max = max(qt.m1$hours))
yrep <- round(mapply(rpois, n = n.obs, lambda = theta.sim*t.sim))
```

##### Minimum

```{r}
obs.min <- min(qt.m1$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)   # simulated minimum
pval <- length(sim.min[sim.min >= obs.min]) / m
```
```{r, echo = FALSE}
h <- hist(sim.min, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,40), ylim = c(0,0.15))
lines(rep(obs.min, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.min, max(h$density),  obs.min, pos = 3, offset = 0.5)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**:  

##### Maximum

```{r}
obs.max <- max(qt.m1$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)   # simulated maximum
pval <- length(sim.max[sim.max <= obs.max]) / m
```
```{r, echo = FALSE}
h <- hist(sim.max, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,80), ylim = c(0, 0.12))
lines(rep(obs.max, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value =", round(pval, 2)), side = 3)
text(obs.max, max(h$density), obs.max, pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Range (Full)

```{r}
obs.range <- diff(range(qt.m1$body.count))
sim.itmd <- apply(yrep, 2, range)
sim.range <- apply(sim.itmd, 2, diff)
pval <- length(sim.range[sim.range <= obs.range]) / m
```
```{r, echo = FALSE}
h <- hist(sim.range, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,80), ylim = c(0,0.12))
lines(rep(obs.range, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.range, max(h$density), obs.range, pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Range (Interquartile)

```{r}
obs.IQRange <- IQR(qt.m1$body.count)
sim.IQRange <- apply(yrep, 2, IQR)
pval <- length(sim.IQRange[sim.IQRange <= obs.IQRange]) / m
```
```{r, echo = FALSE}
h <- hist(sim.IQRange, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(0,30), ylim = c(0,0.2), col = "#1f78b4", border = "#a6cee3")
lines(rep(obs.IQRange, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.IQRange, max(h$density), obs.IQRange, pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Sample Mean

```{r}
obs.mean <- mean(qt.m1$body.count)    # observed mean
sim.mean <- apply(yrep, 2, mean)   # simulated mean
pval <- length(sim.mean[sim.mean >= obs.mean]) / m
```
```{r, echo = FALSE}
h <- hist(sim.mean, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,50), ylim = c(0,0.12))
lines(rep(obs.mean, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.mean, max(h$density), obs.mean, pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Sample Median

```{r}
obs.median <- median(qt.m1$body.count)    # observed median
sim.median <- apply(yrep, 2, median)   # simulated median
pval <- length(sim.median[sim.median >= obs.median]) / m
```
```{r, echo = FALSE}
h <- hist(sim.median, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,50), ylim = c(0,0.12))
lines(rep(obs.median, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.median, max(h$density), obs.median, pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Sample Median Absolute Deviation

```{r}
obs.mad <- mad(qt.m1$body.count)    # observed median
sim.mad <- apply(yrep, 2, mad)   # simulated median
pval <- length(sim.mad[sim.mad >= obs.mad]) / m
```
```{r, echo = FALSE}
h <- hist(sim.mad, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,50), ylim = c(0,0.3))
lines(rep(obs.mad, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval, 2)), side = 3)
text(obs.mad, max(h$density), round(obs.mad, 2), pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

**Conclusion**: 

##### Sample Variance

```{r}
obs.var <- var(qt.m1$body.count)           # observed variance
sim.var <- apply(yrep, 2, var)   # simulated variance
pval <- length(sim.var[sim.var <= obs.var]) / m
```
```{r, echo = FALSE}
h <- hist(sim.var, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,700), ylim = c(0,0.04))
lines(rep(obs.var, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", pval), side = 3)
text(obs.var, max(h$density), round(obs.var, 2), pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```

##### Ratio: Sample Mean / Sample Variance

```{r}
obs.ratio <- obs.mean / obs.var
sim.ratio <- sim.mean / sim.var
pval <- length(sim.ratio[sim.ratio >= obs.ratio]) / m
```
```{r, echo = FALSE}
h <- hist(sim.ratio, breaks = 100, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3")#, xlim = c(0,80), ylim = c(0,0.5))
lines(rep(obs.ratio, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", pval), side = 3)
text(obs.ratio, max(h$density), round(obs.ratio, 2), pos = 3)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density")
```



### Assumption Checking

#### NOTE:
Given the plot structure of movies we would not expect the kill rate to be constant within the movie; however, with the information available we cannot determine whether or not this assumption is violated within each movie.

### Alternative Models

 * Hierachichal model
 * Negative binomial for model 1 in order to handle overdispersion
 * Poisson regression

 
