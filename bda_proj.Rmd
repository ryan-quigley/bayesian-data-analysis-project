---
title: 'MATH 264: Bayesian Project'
author: "Terry Situ, Caie Yan, Ryan Quigley"
geometry: margin=0.5in
output:
  pdf_document: default
  html_notebook: default
  html_document: default
fontsize: 11pt
---

# I. Executive Summary  

Quentin Tarantino’s moves are famous (or notorious) for violent death and massacre. In this project, we use Bayesian method to analyze and predict the body count in Quentin Tarantino’s next movie.  Using a body counts dataset of about 200 movies, we explored the possible factors, such a genre, rating, and year of publication, that could be associated with body count of a movie. Among all the factors, the association of genre with body count is significant.  War movies have much more body counts than other movies. Thus, we decided to build different models for different genres, one for war movies, and two for other movies, such as drama, crime, thriller, western, and action. We used the distribution of the body counts of other movies, except Tarantino’s to specify the parameters of the conjugate prior of different models. We picked Poisson distribution to model the sample data. Detailed justification of using Poisson model is provided later. Our results show that for the same movie duration, if Quentin Tarantino’s next movie is about a war, the body count could be as many as 38 times a non-war related movie.  

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

```{r, echo = FALSE}
## Reading in datasets for the analysis

## Death count data (Quentin Taratino movies removed)
dc <- read.csv("filmdeathcounts_no-qt.csv", 
               header = TRUE, 
               stringsAsFactors = FALSE)
dc$Kill_Rate <- dc$Body_Count/dc$Length_Minutes * 60
## Project dataset meta data
qt <- read.table("qt_data.txt", 
                 header = FALSE, 
                 col.names = c("movie", "body.count", "rating", "hours", "year", "genre"),
                 sep = ";",
                 strip.white = TRUE,
                 stringsAsFactors = FALSE)
qt$kill.rate <- round(qt$body.count / qt$hours, 2)
qt$year <- substr(as.character(qt$year),3,4)
qt$model.1 <- c(1,1,1,0,1,1,0,0,1)
qt$model.2 <- c(0,0,0,1,0,0,0,1,0)
qt$model.3 <- c(0,0,0,0,0,0,1,0,0)
qt <- qt[, c(1,6,5,3,2,4,7,8,9,10)]
```

# II. Data Summary  

##### Taratino Filmography (as writer and director)  

Metadata extracted from IMDB (IMDB, 2016); body count extracted from Vanity Fair infographic (Down for the count, 2013)

```{r, echo = FALSE}
qt.data.sum <- qt[, 1:7]
qt.data.sum <- cbind(c(1,1,1,2,1,1,3,2,1), qt.data.sum)
qt.data.sum$hours <- round(qt.data.sum$hours, 2)
names(qt.data.sum) <- c("Model", "Movie", "Genre", "Year", "Rating", "Body Count", "Hours", "Kill Rate/Hr" )
h8.gsplit <- unlist(strsplit(qt.data.sum[9,3], ", ", fixed = TRUE))
kbv2.gsplit <- unlist(strsplit(qt.data.sum[5,3], ", ", fixed = TRUE))
qt.data.sum[5,3] <- paste(kbv2.gsplit[1:2], collapse = ", ")
qt.data.sum[9,3] <- paste(h8.gsplit[1:3], collapse = ", ")
qt.data.sum <- rbind(qt.data.sum[1:5, ], c("", "", paste(kbv2.gsplit[3:4], collapse = ", "), "", "", "", "", ""), 
                     qt.data.sum[6:9, ], c("", "", paste(h8.gsplit[4:5], collapse = ", "), "", "", "", "", ""))
knitr::kable(qt.data.sum, align = c("l", "l", "l", "l", "r", "r", "r"), row.names = FALSE, digits = 2)
```

##### Historical Movie Sample  

In order to estimate the parameters of the conjugate prior distribution for each model in the next section, we acquired a dataset compiled by Randal Olson from the site http://www.moviebodycounts.com/. See references for links to original dataset. The original data is filtered to make it more relevant to our analysis. The following table summarizes the kill rate per hour of the subset of the historical data used for each model:

```{r, echo = FALSE}
qt.all <- qt
qt.m1 <- qt[qt$model.1 == 1, ]
qt.m2 <- qt[qt$model.2 == 1, ]
qt.m3 <- qt[qt$model.3 == 1, ]

## Genre list for project dataset
qt.g.m1 <- unique(unlist(strsplit(paste(qt.m1$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m2 <- unique(unlist(strsplit(paste(qt.m2$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m3 <- unique(unlist(strsplit(paste(qt.m3$genre, collapse = ", ", sep = ""), ", ")))

## Splitting genre list for each movie in death count dataset
dc.g <- strsplit(tolower(dc$Genre), "|", fixed = TRUE)

## Checking if any genres match those of project genre list
dc$Genre_Match_M1 <- logical(1)
dc$Genre_Match_M2 <- logical(1)
dc$Genre_Match_M3 <- logical(1)
for (i in seq_along(dc.g)) {
  dc$Genre_Match_M1[i] <- (all(dc.g[[i]] %in% qt.g.m1)) | (sum(dc.g[[i]] %in% qt.g.m1) >= 3)
  dc$Genre_Match_M2[i] <- (all(dc.g[[i]] %in% qt.g.m2)) | (sum(dc.g[[i]] %in% qt.g.m2) >= 3)
  dc$Genre_Match_M3[i] <- all(dc.g[[i]] %in% qt.g.m3)
}

## Filter death count dataset
dc.filt.m1 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M1 == TRUE, ]
dc.filt.m2 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M2 == TRUE, ]
dc.filt.m3 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_War == 1, ]
genre.sum <- round(data.frame(1:3, rbind(length(dc.filt.m1$Kill_Rate), length(dc.filt.m2$Kill_Rate), length(dc.filt.m3$Kill_Rate)), 
                        rbind(summary(dc.filt.m1$Kill_Rate), summary(dc.filt.m2$Kill_Rate), summary(dc.filt.m3$Kill_Rate)),
                        row.names = NULL), 2)
names(genre.sum) <- c("Model", "N", "Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.")
knitr::kable(genre.sum, align = "l")
```

# III. Models

From the article where the infographic was originally published:  

> "A few numbers are approximated due to the impossibility of counting precisely how many ninjas are decapitated in **Kill Bill Vol. 1**, how many Nazis are in the theater when it gets set afire in **Inglorious Basterds**, and how many people fall in the never-ending shoot-out scene at the end of **Django Unchained**."  
> - Vanity Fair  

The body count numbers in these three movies are unsually large and require approximation for one main reason: each contains a single scene of unimaginable (to most people except Tarantino) violence that dramatically increasese the death toll. In light of this information, we find it impossible to judge exchangeability for the entire nine movie filmography of Tarantino as both writer and director; however, we are not attempting to use this as evidence for throwing out the data points as outliers. Instead, given our limited knowledge of alternative Bayesian models, we have chosen to group the data into three distinct sets, and we have fit a model for each group. All movies not described in the quote above are grouped into model 1; the other three movies (in bold in the quote) are further separated into two groups based on the binary genre classification war/non-war. This justifcation was based on analysis of the historical movie sample, which indicated that body counts and kill rates per hour were dramatically higher in the war genre than all other genres that Tarantino movies fall under. The movies included in each model are indicated in the *Model* column of the Taratino filmography table above.

The following table summarizes the model components, and provides two posterior summary statistics for $\theta$ (Note: $\Gamma(\alpha, \beta) = \text{Gamma}(\alpha, \beta)$).  

Model | Prior                 |  Likelihood        | Posterior                | Posterior Mode | 99% HPD Inteval
------|-----------------------|--------------------|--------------------------|-------------|-------------------------
 1 | $\Gamma(1.46, 0.053)$  | $\propto \theta^{59}e^{-13.73\theta}$ | $\Gamma(60.46, 13.79)$  | 4.31   | [3.03, 5.92]
 2 | $\Gamma(2.13, 0.064)$  | $\propto \theta^{126}e^{-4.6\theta}$  | $\Gamma(128.13, 4.66)$  | 25.26  | [21.49, 33.97]
 3 | $\Gamma(2.015, 0.023)$ | $\propto \theta^{396}e^{-2.55\theta}$ | $\Gamma(398.015, 2.57)$ | 154. 3 | [135.20, 175.12]

## Sampling Distribution: Poisson with rate and exposure

According to Gelman et. al. (pg. 45), this model is NOT exchangeable in the $y_i$'s but is exchangeable in the pairs $(t,y)_i$

##### Assumptions and Justification

Let $y(t)$ denote the number of events that have occurred during a time interval $[0, t]$  

* P1: $y(0) = 0$
* P2: For all $n \geq 0$, and for any two time intervals, $I_1$ and $I_2$, of equal length, Pr($n$ events in $I_1$) = Pr($n$ events in $I_2$)
* P3: Events that occur in nonoverlapping time intervals are mutually independent (want to relax this and replace with exchangeability)
* P4: $\lim\limits_{h \to 0}\frac{\text{Pr}(y(h) > 1)}{h} = 0$
* P5: $0 < \text{Pr}\{y(t)=0\} < 1, ~ \forall ~  t > 0$

Under these conditions, there exists a positive number $\theta$ that produces the density below where $\theta$ = the true underlying kill rate per hour in Quentin Taratino movies:  
$$p(y ~|~ \theta) = \frac{1}{y!}(\theta t)^{y}e^{-(\theta t)} \cdot {1}_{\{0,1,2,...\}}(y)$$
Given the plot structure of movies we would not expect the kill rate to be constant within the movie; however, with the information available we cannot determine whether or not this assumption is violated within each movie.  

## Likelihood

A necessary and sufficient condition to get product of identical distributions is  
$$p(y_1,...y_n ~|~ s_n) = \frac{s_n!}{y_1!,...,y_n!} \prod\limits_{i=1}^n \left(\frac{1}{n}\right)^{y_i}$$
For every $n$, where $s_n = y_1 + ... + y_n$. This condition is not reasonably justified if we group all the data together; however, once grouped into three models, the condition seems reasonable within each model. Assuming this condition holds, the likelihood is:  
$$p(y ~|~ \theta) \propto \theta^{\left(\sum\limits_{i=1}^n y_i\right)} \exp\left( -\theta \sum\limits_{i=1}^n t_i \right)$$

## Conjugate Prior: Gamma

Since the conjugate prior distribution for the Poisson sampling distribution is the Gamma distribution, the prior distribution of $\theta$ will be of the form:  
$$p(\theta) \propto \theta^{\alpha - 1}e^{-\beta\cdot\theta}$$

The parameters of the conjugate prior distributions for each model were calculated by solving analytically a pair of equations for $\alpha$ and $\beta$ (Lee, 2016). The equations were determined by (1) setting the mode formula for the gamma distribution equal to the mode of the kernel density estimate, and by (2) observing the interval that approximately 99.9% of the historical data subset was contained in. For modes of the kernel density estimates are 8.61, 17.85, and 43.83, respectively. The intervals that appeared to contain 99.9% of the historical data were (0,150), (0,150), and (0,400), respectively. See the *Model Derivations* section of the appendix for the mathematical equations and plots of the kernel density estimates and theoretical gamma distributions.

```{r, echo = FALSE}
## Model 1
h <- 5
alpha.m1 <- 1.46
beta.m1 <- 0.054
d1 <- density(dc.filt.m1$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 160)

## Model 2: 
h <- 10
alpha.m2 <- 2.13
beta.m2 <- 0.064
d2 <- density(dc.filt.m2$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 160)

## Model 3: 
h <- 30
alpha.m3 <- 2.015
beta.m3 <- 0.023
d3 <- density(dc.filt.m3$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 400)
```

## Posterior: Gamma

In general, the posterior distribution is Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$ (Gelman et. al, pg. 45)

```{r, echo = FALSE, fig.width=8, fig.height=3}
par(mfrow = c(1,3))
## Model 1
t.sum.m1 <- sum(qt.m1$hours)
y.sum.m1 <- sum(qt.m1$body.count)
alpha.post.m1 <- alpha.m1 + y.sum.m1
beta.post.m1 <- beta.m1 + t.sum.m1
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), from = 0, to = 20, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0,0.8))
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 20, lty = 2, col = "slateblue", add = TRUE)
#legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")

## Model 2
t.sum.m2 <- sum(qt.m2$hours)
y.sum.m2 <- sum(qt.m2$body.count)
alpha.post.m2 <- alpha.m2 + y.sum.m2
beta.post.m2 <- beta.m2 + t.sum.m2
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), 
      from = 0, to = 100, n = 500, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0, 0.2))
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), 
      from = 0, to = 100, n = 500, lty = 2, col = "slateblue", add = TRUE)
legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")

## Model 3
t.sum.m3 <- sum(qt.m3$hours)
y.sum.m3 <- sum(qt.m3$body.count)
alpha.post.m3 <- alpha.m3 + y.sum.m3
beta.post.m3 <- beta.m3 + t.sum.m3
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), 
      from = 0, to = 300, n = 1000, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0,0.06))
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), 
      from = 0, to = 300, n = 1000, lty = 2, col = "slateblue", add = TRUE)
#legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

```{r, echo = FALSE}
p <- 0.99
hpd.m1 <- HDInterval::hdi(qgamma, shape = alpha.post.m1, rate = beta.post.m1, credMass = p)
hpd.m2 <- HDInterval::hdi(qgamma, shape = alpha.post.m2, rate = beta.post.m2, credMass = p)
hpd.m3 <- HDInterval::hdi(qgamma, shape = alpha.post.m3, rate = beta.post.m3, credMass = p)
hpd.df <- data.frame(1:3, round(rbind(hpd.m1, hpd.m2, hpd.m3), 2), row.names = NULL)
names(hpd.df) <- c("Model", "Lower Bound", "Upper Bound")
# knitr::kable(hpd.df, align = "l")
```

## Posterior Predictive: Negative Binomial

The posterior predictive distribution for a single additional observation is a negative binomial distribution of the form $\text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)$ (Gelman et.al., pg. 44-45). See the *Model Derivations* section of the appendix for details. For a sample of potential movie durations, the most likely body count values broken down by model in Quentin Taratino's next movie are summarized in the following table.  

Model | Movie Duration (hours) | Body Count | $\text{Pr}(\tilde{y} ~\vert~ y)$
------|--------|---------|----------
1       | 1.5 | 6    | 0.15
&nbsp;  | 2   | 8    | 0.127
&nbsp;  | 2.5 | 10   | 0.112
&nbsp;  | 3   | 12   | 0.1
2       | 1.5 | 40   | 0.054
&nbsp;  | 2   | 54   | 0.045
&nbsp;  | 2.5 | 68   | 0.039
&nbsp;  | 3   | 81   | 0.034
3       | 1.5 | 231  | 0.021
&nbsp;  | 2   | 308  | 0.017
&nbsp;  | 2.5 | 385  | 0.014
&nbsp;  | 3   | 462  | 0.013

```{r, echo = FALSE, fig.width=8}
def.par <- par(no.readonly = TRUE)
margins <- par("mar")
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
par(mar = c(3.1,3.1,3.1,2.1))

## Model 1
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd.m1 <- beta.post.m1/t.new
post.preds <- mapply(dnbinom, x = 0:35, MoreArgs = list(size = rep(alpha.post.m1, length(t.new)), prob = (beta.ppd.m1/(beta.ppd.m1 + 1))))
x.modes.m1 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m1 <- post.preds[matrix(c(1:4, x.modes.m1), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 35), ylim = c(0, 0.16))
for (i in 1:4) {
  lines(0:35, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m1[i] - 1, 2), c(-0.1, modes.m1[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,35,5), col = "gray50")
axis(2, seq.int(0,0.16,0.04), col = "gray50")
title(main = "Model 1", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
legend(25, 0.16, title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n", cex = 0.75)
#text(x.modes.m1 + 3, modes.m1 + 0.04, labels = round(modes.m1, 3), pos = 4)
#arrows(x.modes.m1 - 1, modes.m1, x.modes.m1 + 3, modes.m1 + 0.04, length = 0.1, code = 1)
points(x.modes.m1 - 1, rep(0, length(x.modes.m1)), pch = 4, col = "black")


## Model 2
beta.ppd.m2 <- beta.post.m2/t.new
post.preds <- mapply(dnbinom, x = 0:140, MoreArgs = list(size = rep(alpha.post.m2, length(t.new)), prob = (beta.ppd.m2/(beta.ppd.m2 + 1))))
x.modes.m2 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m2 <- post.preds[matrix(c(1:4, x.modes.m2), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 140), ylim = c(0, 0.06))
for (i in 1:4) {
  lines(0:140, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m2[i] - 1, 2), c(-0.1, modes.m2[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,160,40), col = "gray50")
axis(2, seq.int(0,0.06,0.02), col = "gray50")
title(main = "Model 2", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
#legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m2 + 3, modes.m2 + 0.04, labels = round(modes.m2, 3), pos = 4)
#arrows(x.modes.m2 - 1, modes.m2, x.modes.m2 + 3, modes.m2 + 0.04, length = 0.1, code = 1)
points(x.modes.m2 - 1, rep(0, length(x.modes.m2)), pch = 4, col = "black")

beta.ppd.m3 <- beta.post.m3/t.new
post.preds <- mapply(dnbinom, x = 0:600, MoreArgs = list(size = rep(alpha.post.m3, length(t.new)), prob = (beta.ppd.m3/(beta.ppd.m3 + 1))))
x.modes.m3 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m3 <- post.preds[matrix(c(1:4, x.modes.m3), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")


## Model 3
plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(150, 600), ylim = c(0, 0.024))
for (i in 1:4) {
  lines(0:600, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m3[i] - 1, 2), c(-0.1, modes.m3[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,600,100), col = "gray50")
axis(2, seq.int(0,0.024,0.006), col = "gray50")
title(main = "Model 3", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
#legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m3 + 3, modes.m3 + 0.04, labels = round(modes.m3, 3), pos = 4)
#arrows(x.modes.m3 - 1, modes.m3, x.modes.m3 + 3, modes.m3 + 0.04, length = 0.1, code = 1)
points(x.modes.m3 - 1, rep(0, length(x.modes.m3)), pch = 4, col = "black")
par(def.par)
par(mar = margins)
```


```{r, echo = FALSE}
ppd.df <- data.frame(hours = t.new, m1 = (x.modes.m1 - 1), 
                     m1p = round(modes.m1, 3), m2 = (x.modes.m2 - 1), m2p = round(modes.m2, 3), m3 = (x.modes.m3 - 1), m3p = round(modes.m3, 3))
names(ppd.df) <- c("Movie Duration (hours)", "Model 1: Body Count", "Model 1: Pr(y)", "Model 2: Body Count", "Model 2: Pr(y)", "Model 3: Body Count", "Model 3: Pr(y)")
# knitr::kable(ppd.df, align = "l")
```

### Posterior Predictive Checking  

The following plots summarize the results of three test quantities for model 1: the minimum, maximum, and the ratio of the sample variance to the sample mean. Note, only model 1 has enough data points to calculate meaningful test quantities, so posterior predictice checking was not performed for models 2 and 3. The red vertical line in the plots indicates the test quantity value for the observed data; the blue histogram represents the test quantity evaluated for 500,000 replications of simulated data. See the *Model Checking* section of the appendix for details on the simulation procedure and the code used to generate the replications. Observed individually, the plots for minimum and maximum do not suggest strong discrepancies between the model and the observed data, but interpreted together they do raise some concern about the spread of the data. Too much spread in the data suggests overdispersion, which in turn may mean the Poisson model is not appropriate. An interesting feature of the Poisson sampling distribution is that the expectation and variance are equal. Thus, we would expect the ratio of sample variance to sample mean to be around one if the data is truly from the Poisson distribution. The plot of variance-mean ratio shows a significant discrepancy between the observed value and the simulated values. Thus, our model is not capturing the variance in our data well. A model that allows the variance to be fit separately from the mean, such as negative binomial or normal, would likely provide a better fit to our data.

```{r, echo = FALSE}
## Simulation set=up
set.seed(2016)
m <- 500000
n.obs <- length(qt.m1$body.count)
theta.sim <- rgamma(m, shape = alpha.post.m1, rate = beta.post.m1)
t.sim <- runif(m, min = min(qt.m1$hours), max = max(qt.m1$hours))
yrep <- round(mapply(rpois, n = n.obs, lambda = theta.sim*t.sim))

obs.min <- min(qt.m1$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)   # simulated minimum
pval.min <- length(sim.min[sim.min >= obs.min]) / m

obs.max <- max(qt.m1$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)      # simulated maximum
pval.max <- length(sim.max[sim.max <= obs.max]) / m

obs.mean <- mean(qt.m1$body.count)    # observed mean
sim.mean <- apply(yrep, 2, mean)   # simulated mean
pval.mean <- length(sim.mean[sim.mean >= obs.mean]) / m

obs.var <- var(qt.m1$body.count)           # observed variance
sim.var <- apply(yrep, 2, var)   # simulated variance
pval.var <- length(sim.var[sim.var <= obs.var]) / m

obs.ratio <- obs.var / obs.mean
sim.ratio <- sim.var / sim.mean
pval.ratio <- length(sim.ratio[sim.ratio <= obs.ratio]) / m
```

```{r, echo = FALSE, fig.width=8}
def.par <- par(no.readonly = TRUE)
margins <- par("mar")
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
par(mar = c(3.1,3.1,3.1,2.1))

## Var/Mean Ratio
h <- hist(sim.ratio, breaks = 100, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,6), ylim = c(0,1))
lines(rep(obs.ratio, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval.ratio, 2)), side = 3, cex = 0.75)
text(obs.ratio, max(h$density), round(obs.ratio, 2), pos = 3)
title(main = "Var/Mean Ratio", cex.main = 1, line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)

## Minimum
h <- hist(sim.min, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,25), ylim = c(0,0.20))
lines(rep(obs.min, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval.min, 2)), side = 3, cex = 0.75)
text(obs.min, max(h$density),  obs.min, pos = 3, offset = 0.5)
title(main = "Minimum", cex.main = 1, line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)

## Maximum
h <- hist(sim.max, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,40), ylim = c(0, 0.16))
lines(rep(obs.max, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value =", round(pval.max, 2)), side = 3, cex = 0.75)
text(obs.max, max(h$density), obs.max, pos = 3)
title(main = "Maximum", cex.main = 1, line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)
par(def.par)
par(mar = margins)
```

### Conclusions

 * Limitations
 * Alternative Models
    - Hierachichal model
    - Negative binomial for model 1 in order to handle overdispersion
    - Poisson regression

\pagebreak

# V. Appendix 

## Model Derivations

##### Conjugate prior: gamma

The following is a detailed description of the filtering applied to the original historical movie sample. First all movies with Quentin Tarantino as director were removed. This excluded one movie not in the dataset described above where Tarantino was a director but not a writer: Sin City. Next, we restricted the range of years to exclude any movies released prior to 1989. This was done to avoid influence from movies in a time period with dramatically different social views about movie violence. We assumed movies released in the three years prior to the release of his first movie (*Reservoir Dogs*, 1992) would also be similar enough in nature. All of the Tarantino movies have an MPAA rating of R with the exception of *Death Proof*, which was unrated. As a result, we included movies with ratings R or Unrated. The filter conditions discussed so far apply to all three models, but the filtering based on genre is specific to each model. For both model 1 and model 2, the unique set of genres was determined for the data points in each model. For model 1, the set consists of crime, drama, thriller, action, western, mystery; for model 2, action, thriller, drama, western. Using these sets, a movie from the historical sample was included if one of two conditions was met: (1) all its genres matched the unique genre set, or (2) at least 3 of its genres matched the unique genre set. The number of genres listed for a movie can vary quite a bit, so these conditions help prevent the filtering from excluding too many movies. For Model 3, the genre filter condition is simply a check to see if the movie has the genre war. This condition is far less restrictive than those for models 1 and 2, but is necessary due to the small number of war movies with recorded body counts. We suspect this is due to the difficulty and tedium of recording body counts for war movies. One final note: the original historical movie dataset does not contain any movies with zero deaths. For our purposed, this ensures the kill rate per hour is greater than zero, which is appropriate for the support of the Gamma distribution.  

\begin{align}
\text{Mode}(\theta) & = \frac{\alpha - 1}{\beta} \\
0.999 & = \int\limits_0^{u} \frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{\alpha -1}\text{e}^{-\beta}{\theta}
\end{align}

```{r, echo = FALSE, fig.width=8, fig.height=3}
par(mfrow = c(1,3))#, cex.axis = 0.75, cex.main = 0.75, cex.lab = 0.75)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 160), ylim = c(0, 0.05), 
     bty = "n", type = "n")
hist(dc.filt.m1$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d1, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 160, 40))
axis(2, at = seq.int(0, 0.05, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 200, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m1, ", ", beta.m1,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n", cex = 0.75)
title(main = "Model 1", xlab = expression(theta), ylab = "Density")

plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 160), ylim = c(0, 0.05), 
     bty = "n", type = "n")
hist(dc.filt.m2$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d2, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 160, 40))
axis(2, at = seq.int(0, 0.05, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), from = 0, to = 160, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m2, ", ", beta.m2,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n", cex = 0.75)
title(main = "Model 2", xlab = expression(theta), ylab = "Density")

plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 400), ylim = c(0, 0.02), 
     bty = "n", type = "n")
hist(dc.filt.m3$Kill_Rate, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d3, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 400, 100))
axis(2, at = seq.int(0, 0.02, 0.005))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), from = 0, to = 400, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m3, ", ", beta.m3,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n", cex = 0.75)
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

##### Posterior preditive distribution: negative binomial [ADD STEPS!!!]
\begin{align*}
p(\tilde{y} ~|~ y) & = \int\limits_0^{\infty} p(\tilde{y} ~|~ \theta)\cdot p(\theta ~|~ y) ~d\theta \\
& = \int\limits_0^{\infty} \text{Poisson}(\tilde{y}(t) ~|~ \theta) \cdot \text{Gamma}\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right) ~d\theta \\
& = \text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)
\end{align*}

##### Noninformative (Jeffreys) Prior  
\begin{align*}
I_n(\theta) & = \text{E}\left\{ \left(\frac{\partial}{\partial \theta} \log p(y~|~\theta)\right)^2 ~\Bigg|~ \theta \right\} \\
& = \frac{1}{\theta^2} \text{E}\left[\left(\sum\limits_{i=1}^n y_i - \theta\sum\limits_{i=1}^n t_i\right)^2 ~\Bigg|~ \theta \right] \\
& = \frac{1}{\theta^2} \text{Var}\left( \sum\limits_{i=1}^n y_i ~\Bigg|~ \theta \right) && \text{assuming $y_i$ are $c.i.i.d$ given $\theta$} \\
& = \frac{1}{\theta} \cdot \sum\limits_{i=1}^n t_i \\
\implies p(\theta) & \propto \sqrt{\frac{1}{\theta}}
\end{align*} 

## Model Checking

##### Posterior preditive checking: simulation procedure

Because our sampling distribution involves rate and exposure, we must appropriately account for the exposure, $t$, when approximating the distribution of $T(y^{rep}, \theta)$ using simulation. The simulation will be performed as follows:  

1. Draw $\theta^{(i)}$ from Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$
2. Multiply $\theta^{(i)}$ by $t^{*}$
3. For each $(\theta^{(i)} \cdot t^{*})$, simulate a draw $y^{rep(i)}(t^{*})$ from Poisson$(\theta^{(i)} \cdot t^{*})$
4. Calculate $T(y^{rep(i)}(t^{*}), \theta^{(i)})$ for $i$ = 1,..., 500000.

For model 1 mostly, this presents a problem when deciding what value of $t^{*}$ to choose for each simulation. To address this issue, we note that there does not appear to be a strong relationship between body count and movie duration, which is supported by a low correlation value of 0.21 and the plot below. As a result, we draw $t^{*}$ randomly from $U(min\{t_1,...t_n\}, max\{t_1,...t_n\})$ in step 2 of the simulation procedure. In the case of model 1, this is the interval [1.65, 2.78]; for model 2, this is the interval [1.85, 2.75]. For model 3, we simply take $t^{*}$ = 2.55 since the model only has a single data point.

```{r, echo = FALSE}
plot(qt$hours, qt$body.count, ann = FALSE, axes = FALSE, xlim = c(1.5, 3), ylim = c(0,20), col = "violetred", pch = 20)
#box(col = "gray50", lty = 1)
axis(1, at = seq.int(1.5, 3, 0.5), col = "gray50")
axis(2, at = seq.int(0,20,5), col = "gray50")
mtext(paste("Corr(Hours, Body Count) =", round(cor(qt.m1$hours, qt.m1$body.count), 2)), side = 3, cex = 0.75)
title(main = "Model 1", line = 1, cex.main = 1)
title(xlab = "Movie Duration (hours)", ylab = "Body Count")
```

##### Posterior predictive checking: simulation code

```{r, eval = FALSE}
## Model 1
## Simulation set=up
set.seed(2016)
m <- 500000
n.obs <- length(qt.m1$body.count)
theta.sim <- rgamma(m, shape = alpha.post.m1, rate = beta.post.m1)
t.sim <- runif(m, min = min(qt.m1$hours), max = max(qt.m1$hours))
yrep <- round(mapply(rpois, n = n.obs, lambda = theta.sim*t.sim))

## Minimum
obs.min <- min(qt.m1$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)      # simulated minimum
pval.min <- length(sim.min[sim.min >= obs.min]) / m

## Maximum
obs.max <- max(qt.m1$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)      # simulated maximum
pval.max <- length(sim.max[sim.max <= obs.max]) / m

## Ratio: Sample Variance/ Sample Mean
obs.mean <- mean(qt.m1$body.count)  # observed mean
sim.mean <- apply(yrep, 2, mean)    # simulated mean
obs.var <- var(qt.m1$body.count)    # observed variance
sim.var <- apply(yrep, 2, var)      # simulated variance

obs.ratio <- obs.var / obs.mean
sim.ratio <- sim.var / sim.mean
pval.ratio <- length(sim.ratio[sim.ratio <= obs.ratio]) / m
```

##### Sensitivity to Prior Distribution

In this section we consider the effects of the choice of prior distribution on posterior inference, particularly the effects of a noninformative prior instead of the gamma conjugate prior. The Jeffreys prior for $\theta$ follows from the Fisher information, which results in an improper noninformative prior: $p(\theta) \propto \sqrt{\frac{1}{\theta}}$; however, this does not prevent us from finding a posterior distribution for $\theta$ **[WHY???]**. The resulting posterior distribution for $\theta$ is Gamma$\left(0.5 + \sum\limits_{i=1}^n y_i, \sum\limits_{i=1}^n t_i\right)$. Below are the plots of the posterior distributions of $\theta$ as a result of using a conjugate prior and a non-informative prior. In all three models the differences are neglible, so a choosing a non-informative prior instead of a conjugate prior would have little impact on posterior inference.

```{r, echo = FALSE}
par(mfrow = c(1,3))
curve(dgamma(x, shape = y.sum.m1 + 0.5, rate = t.sum.m1), 
      from = 0, to = 10, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(0,10), ylim = c(0,1))
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), 
      from = 0, to = 10, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")


curve(dgamma(x, shape = y.sum.m2 + 0.5, rate = t.sum.m2), 
      from = 10, to = 50, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(10,50), ylim = c(0,0.3))
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), 
      from = 10, to = 50, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
legend("top", legend = c("Conjugate", "Noninformative"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")


curve(dgamma(x, shape = y.sum.m3 + 0.5, rate = t.sum.m3), 
      from = 120, to = 200, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(120,200), ylim = c(0,0.08))
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), 
      from = 120, to = 200, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

\pagebreak

# VI. References

Dr. Lee R code

IMDB. http://www.imdb.com/

http://www.moviebodycounts.com/

Olson, Randy (2013): On-screen movie kill counts for hundreds of films. figshare.
https://dx.doi.org/10.6084/m9.figshare.889719.v1
Retrieved: 01 39, Nov 29, 2016 (GMT)

Vanity Fair. http://www.vanityfair.com/hollywood/2013/02/quentin-tarantino-deaths-movies

Gelman et al.

Poisson distribution. (2016, November 27). In Wikipedia, The Free Encyclopedia. Retrieved 18:33, November 27, 2016, from https://en.wikipedia.org/w/index.php?title=Poisson_distribution&oldid=751763566

Gamma distribution. (2016, November 8). In Wikipedia, The Free Encyclopedia. Retrieved 19:13, November 8, 2016, from https://en.wikipedia.org/w/index.php?title=Gamma_distribution&oldid=748540178
 
The Tarantino Death Toll. https://vimeo.com/148832585