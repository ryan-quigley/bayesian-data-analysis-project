---
title: 'MATH 264: Bayesian Project'
output:
  html_document: default
  html_notebook: default
---

```{r, echo = FALSE}
## Reading in datasets for the analysis

## Death count data (Quentin Taratino movies removed)
dc <- read.csv("filmdeathcounts_no-qt.csv", 
               header = TRUE, 
               stringsAsFactors = FALSE)
dc$Kill_Rate <- dc$Body_Count/dc$Length_Minutes * 60
## Project dataset meta data
qt <- read.table("qt_data.txt", 
                 header = FALSE, 
                 col.names = c("movie", "body.count", "rating", "hours", "year", "genre"),
                 sep = ";",
                 strip.white = TRUE,
                 stringsAsFactors = FALSE)
qt$kill.rate <- round(qt$body.count / qt$hours, 2)
qt <- qt[, c(1,6,5,3,2,4,7)]
```

### Data Summary

##### Quentin Taratino Movie Data  
Meta data extracted from [IMDB](http://www.imdb.com/)

```{r, echo = FALSE}
knitr::kable(qt, align = "l")
qt.all <- qt
```

##### Quentin Taratino Body Count Summary  

```{r, echo = FALSE}
bc.df <- data.frame(rbind(summary(qt$body.count), summary(qt$body.count[qt$body.count != max(qt$body.count)])), 
                    Var = c(var(qt$body.count), var(qt$body.count[qt$body.count != max(qt$body.count)])), 
                    check.names = FALSE, 
                    row.names = c("All Obs.", "Exclude Inglorious Bastards"))
knitr::kable(bc.df, align = "l")
```

Regardless of whether the large outlier associated with *Inglorious Bastards* is included, the sample mean and sample variance differ significantly: this suggests the Poisson model may not be a very good fit.

##### Movie Database Sample  

* filter conditions:
    - year: 1991-2013
    - rating: R, Unrated, or NR
    - genre: crime, drama, thriller, action, adventure, war, western
    - director: not Quentin
* sample size: 244

Kill rate per hour:  
```{r, echo = FALSE}
## Genre list for project dataset
qt.g <- unique(unlist(strsplit(paste(qt$genre, collapse = ", ", sep = ""), ", ")))

## Splitting genre list for each movie in death count dataset
dc.g <- strsplit(tolower(dc$Genre), "|", fixed = TRUE)

## Checking if any genres match those of project genre list
dc$Genre_Match <- logical(1)
for (i in seq_along(dc.g)) {
  dc$Genre_Match[i] <- any(dc.g[[i]] %in% qt.g)
}

## Filter death count dataset
## 1) Rating R or Not Rates
## 2) Year later than 1991 (earliest is 1992)
## 3) Genre Match
dc.filt <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1991 & dc$Genre_Match == TRUE, ]
rownames(dc.filt) <- 1:nrow(dc.filt)
summary(dc.filt$Kill_Rate)
```

### Sampling Distribution: Poisson with rate and exposure

According to Gelman et. al. (pg. 45), this model is NOT exchangeable in the $y_i$'s but is exchangeable in the pairs $(x,y)_i$

##### Assumptions

Lecture (See slide 23 of LectureD):  

* let $y(t)$ denote the number of events that have occurred during a time interval $[0, t]$
* P1: $y(0) = 0$
* P2: For all $n \geq 0$, and for any two time intervals, $I_1$ and $I_2$, of equal length, Pr($n$ events in $I_1$) = Pr($n$ events in $I_2$)
* P3: Events that occur in nonoverlapping time intervals are mutually independent (want to relax this and replace with exchangeability)
* P4: $\lim\limits_{h \to 0}\frac{\text{Pr}(y(h) > 1)}{h} = 0$
* P5: $0 < \text{Pr}\{y(t)=0\} < 1, ~ \forall ~  t > 0$

Under these conditions, there exists a positive number $\theta$ that produces the density below where $\theta$ = the true underlying death rate in Quentin Taratino movies (measured in bodies per hour)

$$p(y ~|~ \theta) = \frac{1}{y!}(\theta t)^{y}e^{-(\theta t)} \cdot {1}_{\{0,1,2,...\}}(y)$$

Wikipedia:  

* $y$ the number of times an event occurs in an interval and it can take values 0, 1, 2,...
* The occurrence of one event does not affect the probability that a second event will occur. That is, events occur independently.
* The rate at which events occur is constant. The rate cannot be higher in some intervals and lower in other intervals.
* Two events cannot occur at exactly the same instant.
* The probability of an event in an interval is proportional to the length of the interval.

Not an assumption but a result of the sampling distribution: E[$y(t) | \theta$] = Var($y(t) | \theta$) = $\theta \cdot t$
    
```{r, echo = FALSE, eval = FALSE}
plot(qt$hours, qt$body.count, ann = FALSE, axes = FALSE, xlim = c(1.5, 3), col = "violetred", pch = 4)
box(col = "gray50", lty = 1)
axis(1, at = seq.int(1.5, 3, 0.5), col = "gray50", lwd = 0, lwd.tick = 1)
axis(2, at = seq.int(0,400,100), col = "gray50", lwd = 0, lwd.tick = 1)
title(xlab = "Movie Duration (hours)", ylab = "Body Count")
```
  
### Likelihood
A necessary and sufficient condition to get product of identical distributions is  
$$p(y_1,...y_n ~|~ s_n) = \frac{s_n!}{y_1!,...,y_n!} \prod\limits_{i=1}^n \left(\frac{1}{n}\right)^{y_i}$$
For every $n$, where $s_n = y_1 + ... + y_n$. Assuming [NEEDS JUSTIFICATION] this condition holds, the likelihood is:

$$\begin{align*}
p(y ~|~ \theta) & \propto \theta^{\left(\sum\limits_{i=1}^n y_i\right)} \exp\left( -\theta \sum\limits_{i=1}^n t_i \right)  \\
& \propto \theta^{563} e^{ -18.1 \cdot \theta}
\end{align*}$$


    
### Conjugate Prior: Gamma

[Interactive conjugate prior graphic](https://ryan-quigley.shinyapps.io/BDAproject/) using [data](https://figshare.com/articles/On_screen_movie_kill_counts_for_hundreds_of_films/889719) compiled by [Randal Olson](http://www.randalolson.com/) from the site: http://www.moviebodycounts.com/  

```{r, echo = FALSE}
h <- 5
alpha <- 1
beta <- 0.023

d <- density(dc.filt$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 400)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 400), ylim = c(0, 0.035), 
     bty = "n", type = "n")
hist(dc.filt$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 400, 50), labels = NA)
axis(1, at = seq.int(0, 400, 100), lwd = 0, lwd.ticks = 0)
axis(2, at = seq.int(0, 0.035, 0.01))
    
## Gamma parameter controls
g.mean <- alpha/beta
g.mode <- (alpha-1)/beta
g.var <- alpha/(beta)^2

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha, rate = beta), from = 0, to = 400, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c("KDE", "Gamma"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(xlab = expression(theta), ylab = "Density")
```

From the density histogram above, the density appears to be monotonically decreasing in the interval $(0,\infty)$ suggesting a shape parameter $\alpha = 1$. The vast majority of the data is less than 300, so we solve the following equation for $\beta$ taking $\alpha = 1$:
$$\begin{align*}
0.999 & = \int\limits_0^{300} \beta e^{-\beta\theta} d\theta \\
& = 1 - e^{-300\beta} \\
\implies \beta & = \frac{-1}{300} \log(0.001) \\
& = 0.023
\end{align*}$$
Therefore, the conjugate prior distribution of $\theta$ is:
$$p(\theta ~|~ \alpha = 1, \beta = 0.023) \propto e^{-(0.023)\cdot\theta}$$

### Posterior Distribution  

In general, the posterior distribution is Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$. Thus, the posterior distribution of $\theta$ is Gamma(564, 18.123)

```{r, echo = FALSE}
t.sum <- sum(qt$hours)
y.sum <- sum(qt$body.count)
alpha.post <- alpha + y.sum
beta.post <- beta + t.sum
curve(dgamma(x, shape = alpha.post, rate = beta.post), from = 0, to = 300, n = 1000, lty = 1, col = "violetred", ann = FALSE, bty = "n")
curve(dgamma(x, shape = alpha, rate = beta), from = 0, to = 300, n = 1000, lty = 2, col = "slateblue", add = TRUE)
legend("top", legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(xlab = expression(theta), ylab = "Density", line = 2)
```

##### HPD

```{r, echo = FALSE}
library(HDInterval)
p <- 0.99
hpd <- hdi(qgamma, shape = alpha + y.sum, rate = beta + t.sum, credMass = p)
cat("The ", 100*attributes(hpd)$credMass, "% HPD Region for death rate (theta) is: [", paste(round(hpd,2), collapse = ", "), "]", sep = "")
```

### Posterior Predictive Distribution

The posterior predictive distribution for a single additional observation is a negative binomial distribution (see Gelman page 44-45). Assuming conditional independence of y and $\tilde{y}$ given $\theta$.  
\begin{align*}
p(\tilde{y} ~|~ y) & = \int\limits_0^{\infty} p(\tilde{y} ~|~ \theta)\cdot p(\theta ~|~ y) ~d\theta \\
& = \int\limits_0^{\infty} \text{Poisson}(\tilde{y}(t) ~|~ \theta) \text{Gamma}\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right) ~d\theta \\
& = \text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)
\end{align*}

```{r, echo = FALSE}
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd <- beta.post/t.new
post.preds <- mapply(dnbinom, x = 0:140, MoreArgs = list(size = rep(alpha.post, length(t.new)), prob = (beta.ppd/(beta.ppd + 1))))
x.modes <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes <- post.preds[matrix(c(1:4, x.modes), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 160), ylim = c(0, 0.08))
for (i in 1:4) {
  points(0:140, post.preds[i, ], pch = 20, col = col.vec[i])
  lines(rep(x.modes[i] - 1, 2), c(-0.1, modes[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,160,20), col = "gray50")
axis(2, seq.int(0,0.08,0.01), col = "gray50")
title(xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), ")", sep = "")))
legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
text(x.modes - 1, modes, labels = round(modes, 3), pos = 3)
points(x.modes - 1, rep(0, length(x.modes)), pch = 4, col = "black")
```

Thus, the most likely values for the body count in Quentin Taratino's next movie are:

```{r, echo = FALSE}
ppd.df <- data.frame(hours = t.new, y = (x.modes - 1), p = round(modes, 2))
names(ppd.df) <- c("Movie Duration (hours)", "Most Likely Value (body count)", "Probability")
knitr::kable(ppd.df, align = "l")
```

### Sensitivity to Prior Distribution

In this section we analyze the effects of different prior distributions on posterior inference. First we consider the effects of a noninformative prior instead of the gamma conjugate prior. The Jeffreys prior for $\theta$ follows from the Fisher information calculation below:  
$$\begin{align*}
I_n(\theta) & = \text{E}\left\{ \left(\frac{\partial}{\partial \theta} \log p(y~|~\theta)\right)^2 ~\Bigg|~ \theta \right\} \\
& = \frac{1}{\theta^2} \text{E}\left[\left(\sum\limits_{i=1}^n y_i - \theta\sum\limits_{i=1}^n t_i\right)^2 ~\Bigg|~ \theta \right] \\
& = \frac{1}{\theta^2} \text{Var}\left( \sum\limits_{i=1}^n y_i ~\Bigg|~ \theta \right) \\
& = \frac{1}{\theta} \cdot \sum\limits_{i=1}^n t_i \\
\implies p(\theta) & \propto \sqrt{\frac{1}{\theta}}
\end{align*}$$  
This results in an improper prior; however, this does not prevent us from finding a posterior distribution for $\theta$ [WHY???]. The resulting posterior distribution for $\theta$ is Gamma$(563.5, 18.1)$. The posterior distribution parameters are nearly identical to those that resulted from a conjugate prior. The two posterior distributions considered in this analysis thus far are plotted below, which illustrates that the two posterior distributions are pratically identical. This suggests that the posterior distribution is dominated by the data and choice of sampling distribution.

```{r, echo = FALSE}
curve(dgamma(x, shape = 563.5, rate = 18.1), 
      from = 20, to = 40, n = 1000, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(20,40), ylim = c(0,0.40))
curve(dgamma(x, shape = alpha.post, rate = beta.post), 
      from = 20, to = 40, n = 1000, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
legend("top", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(xlab = expression(theta), ylab = "Density", line = 2)
```

[TODO!!!]
Additionally, we will examine the effect of choosing a conjugate gamma prior distribution with larger variance to account for higher body counts.

### Posterior Predictive Checking  
The observed data should look plausible under the posterior predictive distribution

```{r}
## Simulation set=up
set.seed(2016)
m <- 100000
n.obs <- length(qt$body.count)
theta.sim <- rgamma(m, shape = alpha.post, rate = beta.post)
```

##### Maximum

```{r}
yrep <- mapply(rpois, n = n.obs, lambda = (theta.sim*qt$hours[qt$body.count == max(qt$body.count)]))
obs.max <- max(qt$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)   # simulated maximum
pval <- length(sim.max[sim.max <= obs.max]) / m
```
```{r, echo = FALSE}
hist(sim.max, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(0,400), ylim = c(0, 0.06))
lines(rep(obs.max, 2), c(0, 0.06), col = "red", lwd = 1.5)
mtext(paste("p-value:",pval), side = 3)
```

**Conclusion**: The maximum of the dataset is inconsistent with our model. [NEED MORE INTERPRETATION/DETAIL]

##### Minimum

```{r}
yrep <- mapply(rpois, n = n.obs, lambda = (theta.sim*qt$hours[qt$body.count == min(qt$body.count)]))
obs.min <- min(qt$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)   # simulated minimum
pval <- length(sim.min[sim.min >= obs.min]) / m
```
```{r, echo = FALSE}
hist(sim.min, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(0,100), ylim = c(0,0.08))
lines(rep(obs.min, 2), c(0, 0.08), col = "red", lwd = 1.5)
mtext(paste("p-value:",pval), side = 3)
```

**Conclusion**: 

##### Sample mean

```{r}
t.mean <- mean(qt$hours)
obs.mean <- mean(qt$body.count)*t.mean    # observed mean
sim.mean <- apply(yrep, 2, mean)*t.mean   # simulated mean
pval <- length(sim.mean[sim.mean >= obs.mean]) / m
```
```{r, echo = FALSE}
hist(sim.mean, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(120,240), ylim = c(0,0.05))
lines(rep(obs.mean, 2), c(0, 0.05), col = "red", lwd = 1.5)
mtext(paste("p-value:", pval), side = 3)
```

**Conclusion**: 

##### Sample variance

```{r}
obs.var <- var(qt$body.count)    # observed variance
sim.var <- apply(yrep*t.mean, 2, var)   # simulated variance
pval <- length(sim.var[sim.var <= obs.var]) / m
```
```{r, echo = FALSE}
hist(sim.var, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(0,3000), ylim = c(0,0.0025))
lines(rep(obs.var, 2), c(0, 0.0025), col = "red", lwd = 1.5)
mtext(paste("p-value:", pval), side = 3)
```

**Conclusion**: 

##### Range of data

```{r}
t.range <- range(qt$hours)
obs.range <- diff(range(qt$body.count))    # observed variance
sim.itmd <- apply(yrep, 2, range)*t.range
sim.range <- apply(sim.itmd, 2, diff) # simulated variance
pval <- length(sim.range[sim.range <= obs.range]) / m
```
```{r, echo = FALSE}
hist(sim.range, right = FALSE, freq = FALSE, ann = FALSE, xlim = c(0,400), ylim = c(0,0.06))
lines(rep(obs.range, 2), c(0, 0.06), col = "red", lwd = 1.5)
mtext(paste("p-value:", pval), side = 3)
```

**Conclusion**: 

##### Ratio: Sample Mean to Sample Variance

```{r}
obs.mean <- mean(qt$body.count)    # observed mean
sim.mean <- apply(yrep, 2, mean)   # simulated mean
pval <- length(sim.mean[sim.mean >= obs.mean]) / m

obs.var <- var(qt$body.count)    # observed variance
sim.var <- apply(yrep, 2, var)   # simulated variance
pval <- length(sim.var[sim.var <= obs.var]) / m
```
```{r, echo = FALSE}

```

**Conclusion**: 

### Assumption Checking

#### NOTE:
Given the plot structure of movies we would not expect the kill rate to be constant within the movie; however, with the information available we cannot determine whether or not this assumption is violated within each movie.

### Alternative Models

Comment on the negative binomial as a robust alternative to poisson.