---
title: "MATH 264: Bayesian Data Analysis Project"
date: "December 1, 2016"
subtitle: "Body Count Analysis of Quentin Tarantino Movies"
output:
  html_document: default
  html_notebook: default
---

\pagebreak

# I. Executive Summary  

Quentin Tarantino’s moves are famous (or notorious) for violent death and massacre. In this project, we use Bayesian methods to analyze and predict the body count in Quentin Tarantino’s next movie: not a simple task for such an eccentric and unpredictable filmmaker. Even though he has a reputation for violence, the amount and type vary widely from movie to movie. With this in mind, we realized that his collection of movies did not all belong to a single population; there is a natural separation within the collection which suggests that the data be separated into three different groups. As a result, we fit three separate models, one for each group. Given the nature of the data, we chose the Poisson model for our sampling distributions. Using an external dataset containing the body counts and metadata of about 200 movies, we explored the possible factors, such a genre, rating, and release year, that could be associated with body count of a movie. Among all the factors, the association of genre with body count is most notably. This allowed us to develop informed conjugate priors relevant to the data within each models. Using the resulting gamma posterior and negative binomial posterior predictive distribution, our models provide predictions for Tarantino's next movie in three different scenarios; however, given that the majority of his movies belong to one group, we predict that the most likely death count of his next movie will be 9 assuming the movie length is 2.3 hours.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = "center", fig.width = 12)
```

```{r, echo = FALSE}
## Reading in datasets for the analysis

## Death count data (Quentin Taratino movies removed)
dc <- read.csv("filmdeathcounts_no-qt.csv", 
               header = TRUE, 
               stringsAsFactors = FALSE)
dc$Kill_Rate <- dc$Body_Count/dc$Length_Minutes * 60
## Project dataset meta data
qt <- read.table("qt_data.txt", 
                 header = FALSE, 
                 col.names = c("movie", "body.count", "rating", "hours", "year", "genre"),
                 sep = ";",
                 strip.white = TRUE,
                 stringsAsFactors = FALSE)
qt$kill.rate <- round(qt$body.count / qt$hours, 2)
qt$year <- substr(as.character(qt$year),3,4)
qt$model.1 <- c(1,1,1,0,1,1,0,0,1)
qt$model.2 <- c(0,0,0,1,0,0,0,1,0)
qt$model.3 <- c(0,0,0,0,0,0,1,0,0)
qt <- qt[, c(1,6,5,3,2,4,7,8,9,10)]
```

# II. Data Summary  

##### Taratino Filmography (as writer and director)  

The table below provides the background information of nine movies directed and written by Tarantino. The movies included in each model are indicated in the *Model* column. Metadata extracted from IMDB (http://www.imdb.com/); body count extracted from Vanity Fair infographic (Beggs & Handy, 2013)

```{r, echo = FALSE}
qt.data.sum <- qt[, 1:7]
qt.data.sum <- cbind(c(1,1,1,2,1,1,3,2,1), qt.data.sum)
qt.data.sum$hours <- round(qt.data.sum$hours, 2)
names(qt.data.sum) <- c("Model", "Movie", "Genre", "Year", "Rating", "Body Count", "Hours", "Kill Rate/Hr" )
h8.gsplit <- unlist(strsplit(qt.data.sum[9,3], ", ", fixed = TRUE))
kbv2.gsplit <- unlist(strsplit(qt.data.sum[5,3], ", ", fixed = TRUE))
qt.data.sum[5,3] <- paste(kbv2.gsplit[1:2], collapse = ", ")
qt.data.sum[9,3] <- paste(h8.gsplit[1:3], collapse = ", ")
qt.data.sum <- rbind(qt.data.sum[1:5, ], c("", "", paste(kbv2.gsplit[3:4], collapse = ", "), "", "", "", "", ""), 
                     qt.data.sum[6:9, ], c("", "", paste(h8.gsplit[4:5], collapse = ", "), "", "", "", "", ""))
knitr::kable(qt.data.sum, align = c("l", "l", "l", "l", "r", "r", "r"), row.names = FALSE, digits = 2)
```

From the article where the infographic was originally published:  

> "A few numbers are approximated due to the impossibility of counting precisely how many ninjas are decapitated in **Kill Bill Vol. 1**, how many Nazis are in the theater when it gets set afire in **Inglorious Basterds**, and how many people fall in the never-ending shoot-out scene at the end of **Django Unchained**."  
> - Vanity Fair  

The body count numbers in these three movies are unsually large and require approximation for one main reason: each contains a single scene of unimaginable (to most people except Tarantino) violence that dramatically increasese the death toll. In light of this information, we find it impossible to judge exchangeability for the entire nine movie filmography of Tarantino as both writer and director; however, we are not attempting to use this as evidence for throwing out the data points as outliers. Instead, given our limited knowledge of alternative Bayesian models, we have chosen to group the data into three distinct sets, and we have fit a model for each group. All movies not described in the quote above are grouped into model 1; the other three movies (in bold in the quote) are further separated into two groups based on the binary genre classification war/non-war. This justifcation was based on analysis of the historical movie sample, which indicated that body counts and kill rates per hour were dramatically higher in the war genre than all other genres that Tarantino movies fall under. The movies included in each model are indicated in the *Model* column of the Taratino filmography table above.

```{r, echo = FALSE}
qt.all <- qt
qt.m1 <- qt[qt$model.1 == 1, ]
qt.m2 <- qt[qt$model.2 == 1, ]
qt.m3 <- qt[qt$model.3 == 1, ]

## Genre list for project dataset
qt.g.m1 <- unique(unlist(strsplit(paste(qt.m1$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m2 <- unique(unlist(strsplit(paste(qt.m2$genre, collapse = ", ", sep = ""), ", ")))
qt.g.m3 <- unique(unlist(strsplit(paste(qt.m3$genre, collapse = ", ", sep = ""), ", ")))

## Splitting genre list for each movie in death count dataset
dc.g <- strsplit(tolower(dc$Genre), "|", fixed = TRUE)

## Checking if any genres match those of project genre list
dc$Genre_Match_M1 <- logical(1)
dc$Genre_Match_M2 <- logical(1)
dc$Genre_Match_M3 <- logical(1)
for (i in seq_along(dc.g)) {
  dc$Genre_Match_M1[i] <- (all(dc.g[[i]] %in% qt.g.m1)) | (sum(dc.g[[i]] %in% qt.g.m1) >= 3)
  dc$Genre_Match_M2[i] <- (all(dc.g[[i]] %in% qt.g.m2)) | (sum(dc.g[[i]] %in% qt.g.m2) >= 3)
  dc$Genre_Match_M3[i] <- all(dc.g[[i]] %in% qt.g.m3)
}

## Filter death count dataset
dc.filt.m1 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M1 == TRUE, ]
dc.filt.m2 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_Match_M2 == TRUE, ]
dc.filt.m3 <- dc[dc$MPAA_Rating %in% c("R", "Unrated", "NR") & dc$Year > 1989 & dc$Genre_War == 1, ]
genre.sum <- round(data.frame(1:3, rbind(length(dc.filt.m1$Kill_Rate), length(dc.filt.m2$Kill_Rate), length(dc.filt.m3$Kill_Rate)), 
                        rbind(summary(dc.filt.m1$Kill_Rate), summary(dc.filt.m2$Kill_Rate), summary(dc.filt.m3$Kill_Rate)),
                        row.names = NULL), 2)
names(genre.sum) <- c("Model", "N", "Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.")
```

# III. Models

The following table summarizes the model components, and provides two posterior summary statistics for $\theta$ (Note: $\Gamma(\alpha, \beta) = \text{Gamma}(\alpha, \beta)$).  

Model | Prior                 |  Likelihood        | Posterior                | Posterior Mode | 99% HPD Inteval
------|-----------------------|--------------------|--------------------------|-------------|-------------------------
 1 | $\Gamma(1.46, 0.053)$  | $\propto \theta^{59}e^{-13.73\theta}$ | $\Gamma(60.46, 13.79)$  | 4.31   | [3.03, 5.92]
 2 | $\Gamma(2.13, 0.064)$  | $\propto \theta^{126}e^{-4.6\theta}$  | $\Gamma(128.13, 4.66)$  | 25.26  | [21.49, 33.97]
 3 | $\Gamma(2.015, 0.023)$ | $\propto \theta^{396}e^{-2.55\theta}$ | $\Gamma(398.015, 2.57)$ | 154. 3 | [135.20, 175.12]

## Sampling Distribution: Poisson (Rate and Exposure)

The Poisson model parameterized in terms of rate and exposure is,  
$$p(y ~|~ \theta) = \frac{1}{y!}(\theta t)^{y}e^{-(\theta t)} \cdot {1}_{\{0,1,2,...\}}(y)$$  
For our analysis, $\theta$ is the uknown kill rate per hour. According to Gelman et al. (2003, pg. 45), this model is not exchangeable in the $y_i$'s but is exchangeable in the pairs $(t,y)_i$. Due to the genre similarity within each model, we do not have any reservations about judging exchangeability for the data points within each model.

##### Assumptions and Justification

There are a number of assumptions associated with the Poisson model that typically need to be met in order to justify using the model. In the case of observations that represent deaths per movie, at least two of the assumptions appear to be violated. First, imagine splitting a movie into two non-overlapping intervals of equal length such that they cover the entire length of the movie. Based on the general plot structure of movies, the probability that you would have a high number of kills in the first interval would be much lower than the probability of having the same number of kills in the second half, which would likely contain the climax and most of the action. Similarly, a low number of kills would be more likely in the first half than the second. Thus, the probability of $n$ kills in the first interval does not equal the probability of $n$ kills in the second interval.  
Now imagine taking two non-overlapping but back-to-back intervals during the climax of a movie. If people start getting killed in the first interval, the chances of continued killing in the next interval will be significantly increased. Thus, we cannot reasonably assume non-overlapping are mutually indepedent.  

Additionally, the assumption that the simultaneous occurrence of two or more events is impossible seems tenous. For example, movie explosions seem to imply that multiple people die instantenously and silmutaneously. We admit an argument could be made for differences in nanoseconds, but it would be nearly impossible to measure and verify.  

Nevertheless, the overall validity of the assumptions is questionable enough to preclude the use of the Poisson model based on these assumptions. Therefore, we turn to the General Representation Theorem, with the additional necessary and sufficient condition developed by Freedman, in order justify that the observations are conditionally indepedent given $\theta$.  

For model 1, if we imagine dropping 59 balls into 6 boxes each with probability 1/6, the result (11, 7, 4, 13, 6, 18) seems completely reasonable. Similarly for model 2, (62, 64) is a likely result of dropping 126 balls into 2 bins with 1/2 probability each. Model 3 does not require this justification because it only has one data point. To provide a more concrete justification, we simulated 10,000 draws from the multinomial distribution for each model and plotted the results in the section *Multinomial Condition Check* of the appendix. The plots further support our conclusion that the condition is met. Therefore, the likehood for each model is of the form  $p(y ~|~ \theta) \propto \theta^{\left(\sum\limits_{i=1}^n y_i\right)} \exp\left( -\theta \sum\limits_{i=1}^n t_i \right)$

## Conjugate Prior: Gamma

We chose to use conjugate priors for our models because data on body counts per movie was readily available thanks to the dedication of movie buffs on the internet. Since the conjugate prior distribution for the Poisson sampling distribution is the Gamma distribution, the prior distribution of $\theta$ will be of the form $p(\theta) \propto \theta^{\alpha - 1}e^{-\beta\cdot\theta}$

In order to estimate the parameters of the conjugate prior distribution for each model in the next section, we acquired a dataset compiled by Randal Olson from the site http://www.moviebodycounts.com/. See references for links to original dataset, and the *[Model Derivations]* section of the appendix for a detailed summary of the historical data used. The parameters of the conjugate prior distributions for each model were calculated by solving analytically a pair of equations for $\alpha$ and $\beta$ (Lee, 2016). The equations were determined by (1) setting the mode formula for the gamma distribution equal to the mode of the kernel density estimate, and by (2) observing the interval that approximately 99.9% of the historical data subset was contained in. For each model, the modes of the kernel density estimates are 8.61, 17.85, and 43.83, respectively. The intervals that appeared to contain 99.9% of the historical data were (0,150), (0,150), and (0,400), respectively.

```{r, echo = FALSE}
## Model 1
h <- 5
alpha.m1 <- 1.46
beta.m1 <- 0.054
d1 <- density(dc.filt.m1$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 160)

## Model 2: 
h <- 10
alpha.m2 <- 2.13
beta.m2 <- 0.064
d2 <- density(dc.filt.m2$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 160)

## Model 3: 
h <- 30
alpha.m3 <- 2.015
beta.m3 <- 0.023
d3 <- density(dc.filt.m3$Kill_Rate, kernel = "epanechnikov", bw = h, from = 0, to = 400)
```

## Posterior: Gamma

In general, the posterior distribution is Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$ (Gelman et al., 2003, pg. 45). The parameterization of each posterior distribution is summarized in the table at the top of this section, and plots of the prior and posterior are included below for additional clarity. Please take note of the different scale of the vertical axis in each plot. For a closer look at the prior distributions for each model see the *Model Derivations* section of the appendix.

```{r, echo = FALSE}
par(mfrow = c(1,3))
## Model 1
t.sum.m1 <- sum(qt.m1$hours)
y.sum.m1 <- sum(qt.m1$body.count)
alpha.post.m1 <- alpha.m1 + y.sum.m1
beta.post.m1 <- beta.m1 + t.sum.m1
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), from = 0, to = 20, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0,0.8))
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 20, lty = 2, col = "slateblue", add = TRUE)
#legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")

## Model 2
t.sum.m2 <- sum(qt.m2$hours)
y.sum.m2 <- sum(qt.m2$body.count)
alpha.post.m2 <- alpha.m2 + y.sum.m2
beta.post.m2 <- beta.m2 + t.sum.m2
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), 
      from = 0, to = 100, n = 500, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0, 0.2))
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), 
      from = 0, to = 100, n = 500, lty = 2, col = "slateblue", add = TRUE)
legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")

## Model 3
t.sum.m3 <- sum(qt.m3$hours)
y.sum.m3 <- sum(qt.m3$body.count)
alpha.post.m3 <- alpha.m3 + y.sum.m3
beta.post.m3 <- beta.m3 + t.sum.m3
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), 
      from = 0, to = 300, n = 1000, lty = 1, col = "violetred", ann = FALSE, bty = "n", ylim = c(0,0.06))
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), 
      from = 0, to = 300, n = 1000, lty = 2, col = "slateblue", add = TRUE)
#legend("topright", inset = 0.05, legend = c("Prior", "Posterior"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

```{r, echo = FALSE}
p <- 0.99
hpd.m1 <- HDInterval::hdi(qgamma, shape = alpha.post.m1, rate = beta.post.m1, credMass = p)
hpd.m2 <- HDInterval::hdi(qgamma, shape = alpha.post.m2, rate = beta.post.m2, credMass = p)
hpd.m3 <- HDInterval::hdi(qgamma, shape = alpha.post.m3, rate = beta.post.m3, credMass = p)
hpd.df <- data.frame(1:3, round(rbind(hpd.m1, hpd.m2, hpd.m3), 2), row.names = NULL)
names(hpd.df) <- c("Model", "Lower Bound", "Upper Bound")
# knitr::kable(hpd.df, align = "l")
```

## Posterior Predictive: Negative Binomial

The posterior predictive distribution for a single additional observation is a negative binomial distribution of the form $\text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)$ (Gelman et al., 2003, pg. 44-45). See the *Model Derivations* section of the appendix for details. Taking the average movie duration for each model, the most likely body count values broken down by model in Quentin Taratino's next movie are summarized in the following table.  

Model | Movie Duration (hours) | Body Count | $\text{Pr}(\tilde{y} ~\vert~ y)$
------|--------|---------|----------
1     | 2.3    | 9    | 0.117
2     | 2.3    | 62   | 0.041
3     | 2.5    | 385  | 0.014

The following plots show the full posterior predictive distribution of each model for a sample of easily interpretable movie durations. For each model, the spread of the distribution increases with movie duration meaning our predictions for the next movie are less precise as movie duration increases.

```{r, echo = FALSE}
def.par <- par(no.readonly = TRUE)
margins <- par("mar")
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
par(mar = c(3.1,3.1,3.1,2.1))

## Model 1
t.new <- seq.int(1.5, 3, 0.5)
beta.ppd.m1 <- beta.post.m1/t.new
post.preds <- mapply(dnbinom, x = 0:35, MoreArgs = list(size = rep(alpha.post.m1, length(t.new)), prob = (beta.ppd.m1/(beta.ppd.m1 + 1))))
x.modes.m1 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m1 <- post.preds[matrix(c(1:4, x.modes.m1), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 35), ylim = c(0, 0.16))
for (i in 1:4) {
  lines(0:35, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m1[i] - 1, 2), c(-0.1, modes.m1[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,35,5), col = "gray50")
axis(2, seq.int(0,0.16,0.04), col = "gray50")
title(main = "Model 1", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
legend(25, 0.16, title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m1 + 3, modes.m1 + 0.04, labels = round(modes.m1, 3), pos = 4)
#arrows(x.modes.m1 - 1, modes.m1, x.modes.m1 + 3, modes.m1 + 0.04, length = 0.1, code = 1)
points(x.modes.m1 - 1, rep(0, length(x.modes.m1)), pch = 4, col = "black")


## Model 2
beta.ppd.m2 <- beta.post.m2/t.new
post.preds <- mapply(dnbinom, x = 0:140, MoreArgs = list(size = rep(alpha.post.m2, length(t.new)), prob = (beta.ppd.m2/(beta.ppd.m2 + 1))))
x.modes.m2 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m2 <- post.preds[matrix(c(1:4, x.modes.m2), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")

plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(0, 140), ylim = c(0, 0.06))
for (i in 1:4) {
  lines(0:140, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m2[i] - 1, 2), c(-0.1, modes.m2[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,160,40), col = "gray50")
axis(2, seq.int(0,0.06,0.02), col = "gray50")
title(main = "Model 2", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
#legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m2 + 3, modes.m2 + 0.04, labels = round(modes.m2, 3), pos = 4)
#arrows(x.modes.m2 - 1, modes.m2, x.modes.m2 + 3, modes.m2 + 0.04, length = 0.1, code = 1)
points(x.modes.m2 - 1, rep(0, length(x.modes.m2)), pch = 4, col = "black")

beta.ppd.m3 <- beta.post.m3/t.new
post.preds <- mapply(dnbinom, x = 0:600, MoreArgs = list(size = rep(alpha.post.m3, length(t.new)), prob = (beta.ppd.m3/(beta.ppd.m3 + 1))))
x.modes.m3 <- apply(post.preds, 1, order, decreasing = TRUE)[1, ]
modes.m3 <- post.preds[matrix(c(1:4, x.modes.m3), nrow = 4)]
post.preds[post.preds < 1e-05] <- NA
col.vec <- c("violetred", "slateblue", "#d95f02", "#1b9e77")


## Model 3
plot(1, 1, ann = FALSE, axes = FALSE, type = "n", lty = 3, col = "grey50", bty = "n", xlim = c(150, 600), ylim = c(0, 0.024))
for (i in 1:4) {
  lines(0:600, post.preds[i, ], pch = 20, col = col.vec[i], type = "b", lwd = 0.5)
  lines(rep(x.modes.m3[i] - 1, 2), c(-0.1, modes.m3[i]), col = "grey50", lty = 3)
}
axis(1, seq.int(0,600,100), col = "gray50")
axis(2, seq.int(0,0.024,0.006), col = "gray50")
title(main = "Model 3", xlab = expression(tilde(y)), ylab = expression(paste("Pr(", tilde(y), " | y)", sep = "")), line = 2)
#legend("topright", title = "Movie Duration (hours)", legend = t.new, col = col.vec, pch = 20, ncol = 2, bty = "n")
#text(x.modes.m3 + 3, modes.m3 + 0.04, labels = round(modes.m3, 3), pos = 4)
#arrows(x.modes.m3 - 1, modes.m3, x.modes.m3 + 3, modes.m3 + 0.04, length = 0.1, code = 1)
points(x.modes.m3 - 1, rep(0, length(x.modes.m3)), pch = 4, col = "black")
par(def.par)
par(mar = margins)
```


```{r, echo = FALSE}
ppd.df <- data.frame(hours = t.new, m1 = (x.modes.m1 - 1), 
                     m1p = round(modes.m1, 3), m2 = (x.modes.m2 - 1), m2p = round(modes.m2, 3), m3 = (x.modes.m3 - 1), m3p = round(modes.m3, 3))
names(ppd.df) <- c("Movie Duration (hours)", "Model 1: Body Count", "Model 1: Pr(y)", "Model 2: Body Count", "Model 2: Pr(y)", "Model 3: Body Count", "Model 3: Pr(y)")
# knitr::kable(ppd.df, align = "l")
```

### Posterior Predictive Checking  

The following plots summarize the results of three test quantities for model 1: the minimum, maximum, and the ratio of the sample variance to the sample mean. Note, only model 1 has enough data points to calculate meaningful test quantities, so posterior predictive checking was not performed for models 2 and 3. The red vertical line in the plots indicates the test quantity value for the observed data; the blue histogram represents the test quantity evaluated for 500,000 replications of simulated data. See the *Model Checking* section of the appendix for details on the simulation procedure and the code used to generate the replications. Observed individually, the plots for minimum and maximum do not suggest strong discrepancies between the model and the observed data, but interpreted together they do raise some concern about the spread of the data. Too much spread in the data suggests overdispersion, which in turn may mean the Poisson model is not appropriate. An interesting feature of the Poisson sampling distribution is that the expectation and variance are equal. Thus, we would expect the ratio of sample variance to sample mean to be around one if the data is truly from the Poisson distribution. The plot of variance-mean ratio shows a significant discrepancy between the observed value and the simulated values. Thus, our model is not capturing the variance in our data well. A model that allows the variance to be fit separately from the mean, such as negative binomial or normal, would likely provide a better fit to our data.

```{r, echo = FALSE}
## Simulation set=up
set.seed(2016)
m <- 500000
n.obs <- length(qt.m1$body.count)
theta.sim <- rgamma(m, shape = alpha.post.m1, rate = beta.post.m1)
t.sim <- runif(m, min = min(qt.m1$hours), max = max(qt.m1$hours))
yrep <- round(mapply(rpois, n = n.obs, lambda = theta.sim*t.sim))

obs.min <- min(qt.m1$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)   # simulated minimum
pval.min <- length(sim.min[sim.min >= obs.min]) / m

obs.max <- max(qt.m1$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)      # simulated maximum
pval.max <- length(sim.max[sim.max <= obs.max]) / m

obs.mean <- mean(qt.m1$body.count)    # observed mean
sim.mean <- apply(yrep, 2, mean)   # simulated mean
pval.mean <- length(sim.mean[sim.mean >= obs.mean]) / m

obs.var <- var(qt.m1$body.count)           # observed variance
sim.var <- apply(yrep, 2, var)   # simulated variance
pval.var <- length(sim.var[sim.var <= obs.var]) / m

obs.ratio <- obs.var / obs.mean
sim.ratio <- sim.var / sim.mean
pval.ratio <- length(sim.ratio[sim.ratio <= obs.ratio]) / m
```

```{r, echo = FALSE}
def.par <- par(no.readonly = TRUE)
margins <- par("mar")
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
par(mar = c(3.1,3.1,3.1,2.1))

## Var/Mean Ratio
h <- hist(sim.ratio, breaks = 100, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,6), ylim = c(0,1))
lines(rep(obs.ratio, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval.ratio, 2)), side = 3, cex = 0.75)
text(obs.ratio, max(h$density), round(obs.ratio, 2), pos = 3)
title(main = "Var/Mean Ratio", line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)

## Minimum
h <- hist(sim.min, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,25), ylim = c(0,0.20))
lines(rep(obs.min, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value:", round(pval.min, 2)), side = 3, cex = 0.75)
text(obs.min, max(h$density),  obs.min, pos = 3, offset = 0.5)
title(main = "Minimum", line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)

## Maximum
h <- hist(sim.max, right = FALSE, freq = FALSE, ann = FALSE, col = "#1f78b4", border = "#a6cee3", xlim = c(0,40), ylim = c(0, 0.16))
lines(rep(obs.max, 2), c(0, max(h$density)), col = "#e31a1c", lwd = 1.5)
mtext(paste("p-value =", round(pval.max, 2)), side = 3, cex = 0.75)
text(obs.max, max(h$density), obs.max, pos = 3)
title(main = "Maximum", line = 1)
title(xlab = expression(paste("T(y, ", theta, ")", sep = "")), ylab = "Density", line = 2)
par(def.par)
par(mar = margins)
```

### Conclusions

The approach we took for this analysis is unorthodox: it is uncommon to break a dataset into distinct groups and fit a model to each individual group. However the dataset is peculiar and it demands special treatment, perhaps using more advanced Bayesian modelling methods unknown to us. For example, we know the negative binomial sampling distribution can be used as a robust alternative to the Poisson distribution to help account for the overdispersion of the data. Since we have presented an argument for grouping the data, it also seems reasonable to assume that a hierarchichal model could fit the data particularly well.  

If readers are unconformtable with the data grouping decision, they may be more accepting of model 1 if viewed as a model for the entire data set where three large outliers have been removed. The parameters of the distributions and the predicted values will be the same, but the interpretation will be slightly different. That being said, we believe our justification for breaking up the dataset is sound given the knowledge we have about the data. Observing only the body count numbers of Tarantino movies, it is clear that the majority of the data is clustered around 10 deaths; that is, the majority of his movies do not have epic scenes of death where the bodies fall so readily that is is difficult to count. Intuitively, it seems likely that his next movie will not be dominated by an epic death scene. Thus, our prediction from model 1 of 9 deaths in the next movie seems reasonable.

But! Tarantino has been quoted saying that he only plans to make 10 movies in total (Robinson, 2013). If we combine the *Kill Bills* into one movie, which most Tarantino fanatics do, that puts us at 8 movies produced so far. Could he end his saga with another two volume film where he pulls out all the stops and introduces more violence and death than ever before? Quite possibly. Thus, its nice to know we have a model to address this possibility, and model 2 tells us that 62 deaths is the most likely value in this case. And finally, if he attempts another war themed blood bath, then model 3 tells us 382 deaths is most likely what we are going to get so we better prepare ourselves for a gore fest.

\pagebreak

# IV. Appendix 

## Table of Posterior Predictions for Next Movie

For a sample of potential movie durations, the most likely body count values broken down by model in Quentin Taratino's next movie are summarized in the following table.  

Model | Movie Duration (hours) | Body Count | $\text{Pr}(\tilde{y} ~\vert~ y)$
------|--------|---------|----------
1       | 1.5 | 6    | 0.15
&nbsp;  | 2   | 8    | 0.127
&nbsp;  | 2.5 | 10   | 0.112
&nbsp;  | 3   | 12   | 0.1
2       | 1.5 | 40   | 0.054
&nbsp;  | 2   | 54   | 0.045
&nbsp;  | 2.5 | 68   | 0.039
&nbsp;  | 3   | 81   | 0.034
3       | 1.5 | 231  | 0.021
&nbsp;  | 2   | 308  | 0.017
&nbsp;  | 2.5 | 385  | 0.014
&nbsp;  | 3   | 462  | 0.013

## Multinomial Condition Check

In order to justify that $y_i,...,y_n$ are c.i.i.d poisson observations given $\theta$, a necessary and sufficient condition is that
$$p(y_1,...y_n ~|~ s_n) = \frac{s_n!}{y_1!,...,y_n!} \prod\limits_{i=1}^n \left(\frac{1}{n}\right)^{y_i}$$
for every $n$, where $s_n = y_1 + ... + y_n$.  

Note, this condition is not reasonably justified if we group all the data together; however, once grouped into three models, the condition seems reasonable within each model. The following plot displays 10,000 simulations of dropping 59 balls in 6 equally likely bins; the black triangles indicate the actually data points used for that model. Note, draws were not simulated for model 2 because it seems obvious that a result of (62, 64) is reasonable when dropping 126 balls into two equally likely bins.

```{r, echo = FALSE}
data.m1 <- c(11,7,4,13,6,18)
llk.check <- rmultinom(n = 100000, prob = rep(1/6, 6), size = 59)
```

```{r, echo = FALSE}
llk.vec <- llk.check
dim(llk.vec) <- NULL
llk.vec <- as.factor(llk.vec)
ct <- table(llk.vec)

# margins <- par("mar")
# par(mar = par("mar") + c(0, 2, 0, 0))

plot(llk.vec)
plot(llk.vec[llk.vec %in% data.m1], add = TRUE, col = "violetred")
legend("topright", inset = 0.05, legend = "Observed Body Count", fill = "violetred", bty = "n")
title(main = c("100,000 Multinomial Draws", "59 bodies 6 movies"), xlab = "Bin Count", ylab = "Frequency")
```

## Model Derivations

##### Conjugate prior: gamma

The following is a detailed description of the filtering applied to the original historical movie sample. First all movies with Quentin Tarantino as director were removed. This excluded one movie not in the dataset described above where Tarantino was a director but not a writer: Sin City. Next, we restricted the range of years to exclude any movies released prior to 1989. This was done to avoid influence from movies in a time period with dramatically different social views about movie violence. We assumed movies released in the three years prior to the release of his first movie (*Reservoir Dogs*, 1992) would also be similar enough in nature. All of the Tarantino movies have an MPAA rating of R with the exception of *Death Proof*, which was unrated. As a result, we included movies with ratings R or Unrated. The filter conditions discussed so far apply to all three models, but the filtering based on genre is specific to each model. For both model 1 and model 2, the unique set of genres was determined for the data points in each model. For model 1, the set consists of *crime, drama, thriller, action, western, mystery*; for model 2, *action, thriller, drama, western*. Using these sets, a movie from the historical sample was included if one of two conditions was met: (1) all its genres matched the unique genre set, or (2) at least 3 of its genres matched the unique genre set. The number of genres listed for a movie can vary quite a bit, so these conditions help prevent the filtering from excluding too many movies. For Model 3, the genre filter condition is simply a check to see if the movie has the genre *war*. This condition is far less restrictive than those for models 1 and 2, but is necessary due to the small number of *war* movies with recorded body counts (we suspect this is due to the difficulty and tedium of recording body counts for *war* movies). The influence *war* has on body count will outweigh the influence of any other genre also listed, so we believe this less restrictive condition for model 3 is not problematic. One final note: the original historical movie dataset does not contain any movies with zero deaths. For our purposes, this ensures the kill rate per hour is greater than zero, which is appropriate for the support of the Gamma distribution. The following table summarizes the kill rate per hour of the subset of the historical data used for each model.

```{r, echo = FALSE}
knitr::kable(genre.sum, align = "l")
``` 

```{r, echo = FALSE}
par(mfrow = c(1,3))#, cex.axis = 0.75, cex.main = 0.75, cex.lab = 0.75)
plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 160), ylim = c(0, 0.05), 
     bty = "n", type = "n")
hist(dc.filt.m1$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d1, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 160, 40))
axis(2, at = seq.int(0, 0.05, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m1, rate = beta.m1), from = 0, to = 200, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m1, ", ", beta.m1,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")

plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 160), ylim = c(0, 0.05), 
     bty = "n", type = "n")
hist(dc.filt.m2$Kill_Rate, breaks = 30, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d2, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 160, 40))
axis(2, at = seq.int(0, 0.05, 0.01))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m2, rate = beta.m2), from = 0, to = 160, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m2, ", ", beta.m2,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")

plot(1,0.01, ann = FALSE, axes = FALSE,
     xlim = c(0, 400), ylim = c(0, 0.02), 
     bty = "n", type = "n")
hist(dc.filt.m3$Kill_Rate, freq = FALSE, axes = FALSE, ann = FALSE, add = TRUE, border = "grey75", col = "grey95")
lines(d3, col = "violetred", lwd = 1.5)
axis(1, at = seq.int(0, 400, 100))
axis(2, at = seq.int(0, 0.02, 0.005))

## Plot theoretical gamma distribution
curve(dgamma(x, shape = alpha.m3, rate = beta.m3), from = 0, to = 400, add = TRUE, lty = 2, col = "slateblue")
#legend("top", legend = paste(c("Mean =", "Mode =", "Variance ="), c(g.mean, g.mode, g.var)), bty = "n")
legend("top", legend = c(paste("Gamma(", alpha.m3, ", ", beta.m3,")", sep = ""), "KDE"), 
       col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

The system of equations used to determine the parameters of the gamma distirubtion from the historical data is,  

\begin{align}
\text{Mode}(\theta) & = \frac{\alpha - 1}{\beta} \\
0.999 & = \int\limits_0^{u} \frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{\alpha -1}\text{e}^{-\beta}{\theta}
\end{align}

##### Posterior preditive distribution: negative binomial

Let $\alpha' = \alpha + \sum\limits_{i=1}^n y_i$ and $\beta' = \beta + \sum\limits_{i=1}^n t_i$
\begin{align*}
p(\tilde{y} ~|~ y) & = \int\limits_0^{\infty} p(\tilde{y} ~|~ \theta)\cdot p(\theta ~|~ y) ~d\theta \\
& = \int\limits_0^{\infty} \text{Poisson}(\tilde{y}(t) ~|~ \theta) \cdot \text{Gamma}\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right) ~d\theta \\
& = \frac{\beta'}{\Gamma(\alpha')} \tilde{t}^{\tilde{y}} \cdot 1_{\{0,1,...\}}(\tilde{y})\int\limits_0^{\infty} \theta^{(\alpha' + \tilde{y}) - 1}e^{-(\beta' + t)\theta}d\theta \\
& = \frac{\beta'}{\Gamma(\alpha')} \tilde{t}^{\tilde{y}} \cdot 1_{\{0,1,...\}}(\tilde{y}) \cdot \frac{\Gamma(\alpha' + \tilde{y})}{(\beta' + t)^{\alpha' + \tilde{y}}} \\
& = \frac{\Gamma(\alpha' + \tilde{y})}{\tilde{y}!\Gamma(\alpha')}\left(\frac{\beta'}{\beta' + \tilde{t}}\right)^{\alpha'}\left(\frac{\tilde{t}}{\beta' + \tilde{t}}\right)^{\tilde{y}} 1_{\{0,1,...\}}(\tilde{y}) \\
& = \frac{\Gamma(\alpha' + \tilde{y})}{\tilde{y}!\Gamma(\alpha')}\left(\frac{\beta'/\tilde{t}}{\beta'/\tilde{t} + 1}\right)^{\alpha'}\left(\frac{1}{\beta'/\tilde{t} + 1}\right)^{\tilde{y}} 1_{\{0,1,...\}}(\tilde{y}) \\
& = \text{NB}\left(\alpha + \sum\limits_{i=1}^n y_i ~,~ \frac{1}{\tilde{t}}\left[\beta + \sum\limits_{i=1}^n t_i \right]\right)
\end{align*}

## Model Checking

##### Posterior preditive checking: simulation procedure

Because our sampling distribution involves rate and exposure, we must appropriately account for the exposure, $t$, when approximating the distribution of $T(y^{rep}, \theta)$ using simulation. The simulation will be performed as follows:  

1. Draw $\theta^{(i)}$ from Gamma$\left(\alpha + \sum\limits_{i=1}^n y_i, \beta + \sum\limits_{i=1}^n t_i\right)$
2. Multiply $\theta^{(i)}$ by $t^{*}$
3. For each $(\theta^{(i)} \cdot t^{*})$, simulate a draw $y^{rep(i)}(t^{*})$ from Poisson$(\theta^{(i)} \cdot t^{*})$
4. Calculate $T(y^{rep(i)}(t^{*}), \theta^{(i)})$ for $i$ = 1,..., 500000.

For model 1, this presents a problem when deciding what value of $t^{*}$ to choose for each simulation. To address this issue, we note that there does not appear to be a strong relationship between body count and movie duration, which is supported by a low correlation value of 0.21 and the plot below. As a result, we draw $t^{*}$ randomly from $U(min\{t_1,...t_n\}, max\{t_1,...t_n\})$ in step 2 of the simulation procedure. In the case of model 1, this is the interval [1.65, 2.78].

```{r, echo = FALSE}
plot(qt$hours, qt$body.count, ann = FALSE, axes = FALSE, xlim = c(1.5, 3), ylim = c(0,20), col = "violetred", pch = 20)
#box(col = "gray50", lty = 1)
axis(1, at = seq.int(1.5, 3, 0.5), col = "gray50")
axis(2, at = seq.int(0,20,5), col = "gray50")
mtext(paste("Corr(Hours, Body Count) =", round(cor(qt.m1$hours, qt.m1$body.count), 2)), side = 3, cex = 0.75)
title(main = "Model 1", line = 1, cex.main = 1)
title(xlab = "Movie Duration (hours)", ylab = "Body Count")
```

##### Posterior predictive checking: simulation code

```{r, eval = FALSE}
## Model 1
## Simulation set=up
set.seed(2016)
m <- 500000
n.obs <- length(qt.m1$body.count)
theta.sim <- rgamma(m, shape = alpha.post.m1, rate = beta.post.m1)
t.sim <- runif(m, min = min(qt.m1$hours), max = max(qt.m1$hours))
yrep <- round(mapply(rpois, n = n.obs, lambda = theta.sim*t.sim))

## Minimum
obs.min <- min(qt.m1$body.count)    # observed minimum
sim.min <- apply(yrep, 2, min)      # simulated minimum
pval.min <- length(sim.min[sim.min >= obs.min]) / m

## Maximum
obs.max <- max(qt.m1$body.count)    # observed maximum
sim.max <- apply(yrep, 2, max)      # simulated maximum
pval.max <- length(sim.max[sim.max <= obs.max]) / m

## Ratio: Sample Variance/ Sample Mean
obs.mean <- mean(qt.m1$body.count)  # observed mean
sim.mean <- apply(yrep, 2, mean)    # simulated mean
obs.var <- var(qt.m1$body.count)    # observed variance
sim.var <- apply(yrep, 2, var)      # simulated variance

obs.ratio <- obs.var / obs.mean
sim.ratio <- sim.var / sim.mean
pval.ratio <- length(sim.ratio[sim.ratio <= obs.ratio]) / m
```

##### Sensitivity to Prior Distribution

In this section we consider the effects of the choice of prior distribution on posterior inference, particularly the effects of a noninformative prior instead of the gamma conjugate prior. The Jeffreys prior for $\theta$ follows from the Fisher information,  
\begin{align*}
I_n(\theta) & = \text{E}\left\{ \left(\frac{\partial}{\partial \theta} \log p(y~|~\theta)\right)^2 ~\Bigg|~ \theta \right\} \\
& = \frac{1}{\theta^2} \text{E}\left[\left(\sum\limits_{i=1}^n y_i - \theta\sum\limits_{i=1}^n t_i\right)^2 ~\Bigg|~ \theta \right] \\
& = \frac{1}{\theta^2} \text{Var}\left( \sum\limits_{i=1}^n y_i ~\Bigg|~ \theta \right) && \text{assuming $y_i$ are $c.i.i.d$ given $\theta$} \\
& = \frac{1}{\theta} \cdot \sum\limits_{i=1}^n t_i \\
\implies p(\theta) & \propto \sqrt{\frac{1}{\theta}}
\end{align*}  
This results in an improper noninformative prior; however, this does not prevent us from finding a posterior distribution for $\theta$. The resulting posterior distribution for $\theta$ is Gamma$\left(0.5 + \sum\limits_{i=1}^n y_i, \sum\limits_{i=1}^n t_i\right)$. Below are the plots of the posterior distributions of $\theta$ as a result of using a conjugate prior and a non-informative prior. In all three models the differences are neglible, so a choosing a non-informative prior instead of a conjugate prior would have little impact on posterior inference.

```{r, echo = FALSE}
par(mfrow = c(1,3))
curve(dgamma(x, shape = y.sum.m1 + 0.5, rate = t.sum.m1), 
      from = 0, to = 10, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(0,10), ylim = c(0,1))
curve(dgamma(x, shape = alpha.post.m1, rate = beta.post.m1), 
      from = 0, to = 10, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 1", xlab = expression(theta), ylab = "Density")


curve(dgamma(x, shape = y.sum.m2 + 0.5, rate = t.sum.m2), 
      from = 10, to = 50, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(10,50), ylim = c(0,0.3))
curve(dgamma(x, shape = alpha.post.m2, rate = beta.post.m2), 
      from = 10, to = 50, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
legend("top", legend = c("Conjugate", "Noninformative"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 2", xlab = expression(theta), ylab = "Density")


curve(dgamma(x, shape = y.sum.m3 + 0.5, rate = t.sum.m3), 
      from = 120, to = 200, 
      lty = 1, col = "violetred", ann = FALSE, bty = "n", xlim = c(120,200), ylim = c(0,0.08))
curve(dgamma(x, shape = alpha.post.m3, rate = beta.post.m3), 
      from = 120, to = 200, 
      lty = "36", lwd = 2, col = "slateblue", add = TRUE)
#legend("topright", legend = c("Posterior (Conjugate Prior)", "Posterior (Noninformative Prior)"), col = c("slateblue", "violetred"), lty = c(2,1), bty = "n")
title(main = "Model 3", xlab = expression(theta), ylab = "Density")
par(mfrow = c(1,1))
```

\pagebreak

# V. References

Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian data analysis (Vol. 2). Boca Raton, FL, USA: Chapman & Hall/CRC.

Lee, B. L. (2016). R code from MATH 264 lectures.

Olson, Randy (2013): On-screen movie kill counts for hundreds of films. figshare.
https://dx.doi.org/10.6084/m9.figshare.889719.v1
Retrieved: 01 39, Nov 29, 2016 (GMT)

Beggs, A. & Handy, B. (2013, February 13). *Down for the count*. Retrieved from
http://www.vanityfair.com/hollywood/2013/02/quentin-tarantino-deaths-movies

Robinson, J. (2016, July 12). *Does Quentin Tarantino Really Have Just Two More Movies in Him?* Retrieved from
http://www.vanityfair.com/hollywood/2016/07/quentin-tarantino-ten-movies-retirement

Poisson distribution. (2016, November 27). In Wikipedia, The Free Encyclopedia. Retrieved 18:33, November 27, 2016, from https://en.wikipedia.org/w/index.php?title=Poisson_distribution&oldid=751763566

Gamma distribution. (2016, November 8). In Wikipedia, The Free Encyclopedia. Retrieved 19:13, November 8, 2016, from https://en.wikipedia.org/w/index.php?title=Gamma_distribution&oldid=748540178
 
Lee, K. B. (2016). *The Tarantino Death Toll* [Video]. Retrieved from https://vimeo.com/148832585